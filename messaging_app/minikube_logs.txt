
==> Audit <==
┌─────────┬────────────────────────────────────────┬──────────┬─────────────────────────┬─────────┬──────────────────────┬──────────────────────┐
│ COMMAND │                  ARGS                  │ PROFILE  │          USER           │ VERSION │      START TIME      │       END TIME       │
├─────────┼────────────────────────────────────────┼──────────┼─────────────────────────┼─────────┼──────────────────────┼──────────────────────┤
│ start   │                                        │ minikube │ DESKTOP-8U9CGS4\ALSAINT │ v1.37.0 │ 19 Oct 25 19:50 CEST │                      │
│ start   │ --driver=docker                        │ minikube │ DESKTOP-8U9CGS4\ALSAINT │ v1.37.0 │ 19 Oct 25 19:52 CEST │                      │
│ start   │ --driver=docker                        │ minikube │ DESKTOP-8U9CGS4\ALSAINT │ v1.37.0 │ 19 Oct 25 19:59 CEST │                      │
│ start   │ --driver=docker                        │ minikube │ DESKTOP-8U9CGS4\ALSAINT │ v1.37.0 │ 20 Oct 25 05:26 CEST │                      │
│ start   │                                        │ minikube │ DESKTOP-8U9CGS4\ALSAINT │ v1.37.0 │ 20 Oct 25 05:33 CEST │                      │
│ start   │ --driver=docker                        │ minikube │ DESKTOP-8U9CGS4\ALSAINT │ v1.37.0 │ 20 Oct 25 06:05 CEST │                      │
│ config  │ set driver docker                      │ minikube │ DESKTOP-8U9CGS4\ALSAINT │ v1.37.0 │ 20 Oct 25 06:07 CEST │ 20 Oct 25 06:07 CEST │
│ delete  │                                        │ minikube │ DESKTOP-8U9CGS4\ALSAINT │ v1.37.0 │ 20 Oct 25 06:07 CEST │ 20 Oct 25 06:08 CEST │
│ start   │ --driver=docker                        │ minikube │ DESKTOP-8U9CGS4\ALSAINT │ v1.37.0 │ 20 Oct 25 06:13 CEST │                      │
│ config  │ view                                   │ minikube │ DESKTOP-8U9CGS4\ALSAINT │ v1.37.0 │ 20 Oct 25 06:17 CEST │ 20 Oct 25 06:17 CEST │
│ start   │ --driver=docker --alsologtostderr -v=7 │ minikube │ DESKTOP-8U9CGS4\ALSAINT │ v1.37.0 │ 20 Oct 25 06:20 CEST │ 20 Oct 25 06:24 CEST │
│ start   │ --driver=docker                        │ minikube │ DESKTOP-8U9CGS4\ALSAINT │ v1.37.0 │ 20 Oct 25 06:29 CEST │ 20 Oct 25 06:30 CEST │
└─────────┴────────────────────────────────────────┴──────────┴─────────────────────────┴─────────┴──────────────────────┴──────────────────────┘


==> Last Start <==
Log file created at: 2025/10/20 06:29:20
Running on machine: DESKTOP-8U9CGS4
Binary: Built with gc go1.24.6 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1020 06:29:20.710182    6760 out.go:360] Setting OutFile to fd 84 ...
I1020 06:29:20.716933    6760 out.go:413] isatty.IsTerminal(84) = true
I1020 06:29:20.716933    6760 out.go:374] Setting ErrFile to fd 88...
I1020 06:29:20.716933    6760 out.go:413] isatty.IsTerminal(88) = true
I1020 06:29:20.744645    6760 out.go:368] Setting JSON to false
I1020 06:29:20.751704    6760 start.go:130] hostinfo: {"hostname":"DESKTOP-8U9CGS4","uptime":5260,"bootTime":1760929300,"procs":306,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.6159 Build 19045.6159","kernelVersion":"10.0.19045.6159 Build 19045.6159","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"cef3d49e-ac67-460e-9e38-cd00866413c4"}
W1020 06:29:20.751704    6760 start.go:138] gopshost.Virtualization returned error: not implemented yet
I1020 06:29:20.817880    6760 out.go:179] 😄  minikube v1.37.0 on Microsoft Windows 10 Pro 10.0.19045.6159 Build 19045.6159
I1020 06:29:20.887302    6760 notify.go:220] Checking for updates...
I1020 06:29:20.890230    6760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1020 06:29:20.912595    6760 driver.go:421] Setting default libvirt URI to qemu:///system
I1020 06:29:21.109878    6760 docker.go:123] docker version: linux-28.5.1:Docker Desktop 4.48.0 (207573)
I1020 06:29:21.149894    6760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1020 06:29:23.252191    6760 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.1022971s)
I1020 06:29:23.253286    6760 info.go:266] docker info: {ID:7741d876-dad4-42dd-9061-63bc2bdba7f8 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:58 OomKillDisable:false NGoroutines:101 SystemTime:2025-10-20 04:29:23.233058158 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8266170368 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.39] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.44] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Users\ALSAINT\.docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-mcp.exe] ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.23.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.42] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1020 06:29:23.359502    6760 out.go:179] ✨  Using the docker driver based on existing profile
I1020 06:29:23.586467    6760 start.go:304] selected driver: docker
I1020 06:29:23.586467    6760 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1020 06:29:23.586467    6760 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1020 06:29:23.651101    6760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1020 06:29:26.033881    6760 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.3827803s)
I1020 06:29:26.033881    6760 info.go:266] docker info: {ID:7741d876-dad4-42dd-9061-63bc2bdba7f8 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:58 OomKillDisable:false NGoroutines:101 SystemTime:2025-10-20 04:29:25.919918715 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8266170368 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.39] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.44] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Users\ALSAINT\.docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-mcp.exe] ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.23.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.42] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1020 06:29:26.426583    6760 cni.go:84] Creating CNI manager for ""
I1020 06:29:26.426630    6760 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1020 06:29:26.426630    6760 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1020 06:29:26.499670    6760 out.go:179] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I1020 06:29:26.627275    6760 cache.go:123] Beginning downloading kic base image for docker with docker
I1020 06:29:26.746327    6760 out.go:179] 🚜  Pulling base image v0.0.48 ...
I1020 06:29:26.859761    6760 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1020 06:29:26.859761    6760 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1020 06:29:26.861256    6760 preload.go:146] Found local preload: C:\Users\ALSAINT\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1020 06:29:26.861256    6760 cache.go:58] Caching tarball of preloaded images
I1020 06:29:26.861777    6760 preload.go:172] Found C:\Users\ALSAINT\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1020 06:29:26.861888    6760 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1020 06:29:26.861888    6760 profile.go:143] Saving config to C:\Users\ALSAINT\.minikube\profiles\minikube\config.json ...
I1020 06:29:27.141908    6760 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon, skipping pull
I1020 06:29:27.141954    6760 cache.go:147] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in daemon, skipping load
I1020 06:29:27.141954    6760 cache.go:232] Successfully downloaded all kic artifacts
I1020 06:29:27.142468    6760 start.go:360] acquireMachinesLock for minikube: {Name:mked5141fcdb92260e32e2d5d454de4c4cd7cad0 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1020 06:29:27.142468    6760 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I1020 06:29:27.142468    6760 start.go:96] Skipping create...Using existing machine configuration
I1020 06:29:27.142468    6760 fix.go:54] fixHost starting: 
I1020 06:29:27.215778    6760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1020 06:29:27.295730    6760 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W1020 06:29:27.295730    6760 fix.go:138] unexpected machine state, will restart: <nil>
I1020 06:29:27.387263    6760 out.go:252] 🏃  Updating the running docker "minikube" container ...
I1020 06:29:27.387263    6760 machine.go:93] provisionDockerMachine start ...
I1020 06:29:27.429670    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:29:27.576225    6760 main.go:141] libmachine: Using SSH client type: native
I1020 06:29:27.587391    6760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe017c0] 0xe04300 <nil>  [] 0s} 127.0.0.1 49971 <nil> <nil>}
I1020 06:29:27.587391    6760 main.go:141] libmachine: About to run SSH command:
hostname
I1020 06:29:27.781218    6760 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1020 06:29:27.781218    6760 ubuntu.go:182] provisioning hostname "minikube"
I1020 06:29:27.837052    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:29:27.937676    6760 main.go:141] libmachine: Using SSH client type: native
I1020 06:29:27.938205    6760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe017c0] 0xe04300 <nil>  [] 0s} 127.0.0.1 49971 <nil> <nil>}
I1020 06:29:27.938205    6760 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1020 06:29:28.177974    6760 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1020 06:29:28.230187    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:29:28.334866    6760 main.go:141] libmachine: Using SSH client type: native
I1020 06:29:28.334866    6760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe017c0] 0xe04300 <nil>  [] 0s} 127.0.0.1 49971 <nil> <nil>}
I1020 06:29:28.334866    6760 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1020 06:29:28.596101    6760 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1020 06:29:28.596208    6760 ubuntu.go:188] set auth options {CertDir:C:\Users\ALSAINT\.minikube CaCertPath:C:\Users\ALSAINT\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\ALSAINT\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\ALSAINT\.minikube\machines\server.pem ServerKeyPath:C:\Users\ALSAINT\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\ALSAINT\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\ALSAINT\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\ALSAINT\.minikube}
I1020 06:29:28.596208    6760 ubuntu.go:190] setting up certificates
I1020 06:29:28.596208    6760 provision.go:84] configureAuth start
I1020 06:29:28.647896    6760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1020 06:29:28.875638    6760 provision.go:143] copyHostCerts
I1020 06:29:28.876679    6760 exec_runner.go:144] found C:\Users\ALSAINT\.minikube/key.pem, removing ...
I1020 06:29:28.876679    6760 exec_runner.go:203] rm: C:\Users\ALSAINT\.minikube\key.pem
I1020 06:29:28.877449    6760 exec_runner.go:151] cp: C:\Users\ALSAINT\.minikube\certs\key.pem --> C:\Users\ALSAINT\.minikube/key.pem (1675 bytes)
I1020 06:29:28.930151    6760 exec_runner.go:144] found C:\Users\ALSAINT\.minikube/ca.pem, removing ...
I1020 06:29:28.930151    6760 exec_runner.go:203] rm: C:\Users\ALSAINT\.minikube\ca.pem
I1020 06:29:28.930670    6760 exec_runner.go:151] cp: C:\Users\ALSAINT\.minikube\certs\ca.pem --> C:\Users\ALSAINT\.minikube/ca.pem (1082 bytes)
I1020 06:29:28.931667    6760 exec_runner.go:144] found C:\Users\ALSAINT\.minikube/cert.pem, removing ...
I1020 06:29:28.931667    6760 exec_runner.go:203] rm: C:\Users\ALSAINT\.minikube\cert.pem
I1020 06:29:28.931974    6760 exec_runner.go:151] cp: C:\Users\ALSAINT\.minikube\certs\cert.pem --> C:\Users\ALSAINT\.minikube/cert.pem (1123 bytes)
I1020 06:29:28.932667    6760 provision.go:117] generating server cert: C:\Users\ALSAINT\.minikube\machines\server.pem ca-key=C:\Users\ALSAINT\.minikube\certs\ca.pem private-key=C:\Users\ALSAINT\.minikube\certs\ca-key.pem org=ALSAINT.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1020 06:29:29.104908    6760 provision.go:177] copyRemoteCerts
I1020 06:29:29.173844    6760 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1020 06:29:29.227019    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:29:29.333984    6760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49971 SSHKeyPath:C:\Users\ALSAINT\.minikube\machines\minikube\id_rsa Username:docker}
I1020 06:29:29.480231    6760 ssh_runner.go:362] scp C:\Users\ALSAINT\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1082 bytes)
I1020 06:29:29.564816    6760 ssh_runner.go:362] scp C:\Users\ALSAINT\.minikube\machines\server.pem --> /etc/docker/server.pem (1180 bytes)
I1020 06:29:29.655082    6760 ssh_runner.go:362] scp C:\Users\ALSAINT\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1020 06:29:29.747484    6760 provision.go:87] duration metric: took 1.1512762s to configureAuth
I1020 06:29:29.747484    6760 ubuntu.go:206] setting minikube options for container-runtime
I1020 06:29:29.748209    6760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1020 06:29:29.817695    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:29:29.974218    6760 main.go:141] libmachine: Using SSH client type: native
I1020 06:29:29.975586    6760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe017c0] 0xe04300 <nil>  [] 0s} 127.0.0.1 49971 <nil> <nil>}
I1020 06:29:29.975610    6760 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1020 06:29:30.172682    6760 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1020 06:29:30.172682    6760 ubuntu.go:71] root file system type: overlay
I1020 06:29:30.172682    6760 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1020 06:29:30.245822    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:29:30.475489    6760 main.go:141] libmachine: Using SSH client type: native
I1020 06:29:30.475970    6760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe017c0] 0xe04300 <nil>  [] 0s} 127.0.0.1 49971 <nil> <nil>}
I1020 06:29:30.475970    6760 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1020 06:29:30.733882    6760 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1020 06:29:30.800489    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:29:30.975147    6760 main.go:141] libmachine: Using SSH client type: native
I1020 06:29:30.975147    6760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe017c0] 0xe04300 <nil>  [] 0s} 127.0.0.1 49971 <nil> <nil>}
I1020 06:29:30.975147    6760 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1020 06:29:31.164298    6760 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1020 06:29:31.164298    6760 machine.go:96] duration metric: took 3.777035s to provisionDockerMachine
I1020 06:29:31.164298    6760 start.go:293] postStartSetup for "minikube" (driver="docker")
I1020 06:29:31.164804    6760 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1020 06:29:31.222191    6760 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1020 06:29:31.254170    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:29:31.318979    6760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49971 SSHKeyPath:C:\Users\ALSAINT\.minikube\machines\minikube\id_rsa Username:docker}
I1020 06:29:31.514809    6760 ssh_runner.go:195] Run: cat /etc/os-release
I1020 06:29:31.527678    6760 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1020 06:29:31.527678    6760 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1020 06:29:31.527678    6760 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1020 06:29:31.527678    6760 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1020 06:29:31.527678    6760 filesync.go:126] Scanning C:\Users\ALSAINT\.minikube\addons for local assets ...
I1020 06:29:31.528715    6760 filesync.go:126] Scanning C:\Users\ALSAINT\.minikube\files for local assets ...
I1020 06:29:31.528715    6760 start.go:296] duration metric: took 364.4169ms for postStartSetup
I1020 06:29:31.585477    6760 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1020 06:29:31.622096    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:29:31.696936    6760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49971 SSHKeyPath:C:\Users\ALSAINT\.minikube\machines\minikube\id_rsa Username:docker}
I1020 06:29:31.880962    6760 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1020 06:29:31.897823    6760 fix.go:56] duration metric: took 4.7553546s for fixHost
I1020 06:29:31.897823    6760 start.go:83] releasing machines lock for "minikube", held for 4.7553546s
I1020 06:29:31.933861    6760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1020 06:29:32.008136    6760 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1020 06:29:32.046705    6760 ssh_runner.go:195] Run: cat /version.json
I1020 06:29:32.077306    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:29:32.139569    6760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49971 SSHKeyPath:C:\Users\ALSAINT\.minikube\machines\minikube\id_rsa Username:docker}
I1020 06:29:32.168107    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:29:32.238022    6760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49971 SSHKeyPath:C:\Users\ALSAINT\.minikube\machines\minikube\id_rsa Username:docker}
I1020 06:29:32.308217    6760 ssh_runner.go:195] Run: systemctl --version
W1020 06:29:32.382409    6760 start.go:868] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1020 06:29:32.383976    6760 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1020 06:29:32.456101    6760 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1020 06:29:32.484226    6760 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1020 06:29:32.540078    6760 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1020 06:29:32.567029    6760 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1020 06:29:32.567029    6760 start.go:495] detecting cgroup driver to use...
I1020 06:29:32.567029    6760 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1020 06:29:32.567568    6760 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1020 06:29:32.663406    6760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1020 06:29:32.743036    6760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1020 06:29:32.767739    6760 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1020 06:29:32.839377    6760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1020 06:29:32.941974    6760 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
W1020 06:29:33.039119    6760 out.go:285] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1020 06:29:33.040701    6760 out.go:285] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1020 06:29:33.042676    6760 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1020 06:29:33.151103    6760 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1020 06:29:33.257297    6760 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1020 06:29:33.349180    6760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1020 06:29:33.435269    6760 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1020 06:29:33.533030    6760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1020 06:29:33.630606    6760 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1020 06:29:33.741840    6760 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1020 06:29:33.844701    6760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1020 06:29:34.165290    6760 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1020 06:29:34.735994    6760 start.go:495] detecting cgroup driver to use...
I1020 06:29:34.736515    6760 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1020 06:29:34.799289    6760 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1020 06:29:34.892060    6760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1020 06:29:34.986800    6760 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1020 06:29:35.108102    6760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1020 06:29:35.203765    6760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1020 06:29:35.232745    6760 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1020 06:29:35.346134    6760 ssh_runner.go:195] Run: which cri-dockerd
I1020 06:29:35.422297    6760 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1020 06:29:35.444594    6760 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1020 06:29:35.539957    6760 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1020 06:29:35.779850    6760 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1020 06:29:35.963437    6760 docker.go:575] configuring docker to use "cgroupfs" as cgroup driver...
I1020 06:29:35.963437    6760 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1020 06:29:36.056901    6760 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1020 06:29:36.150646    6760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1020 06:29:36.431170    6760 ssh_runner.go:195] Run: sudo systemctl restart docker
I1020 06:30:11.033821    6760 ssh_runner.go:235] Completed: sudo systemctl restart docker: (34.6026506s)
I1020 06:30:11.123486    6760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1020 06:30:11.312709    6760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1020 06:30:11.436791    6760 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1020 06:30:11.558189    6760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1020 06:30:11.670658    6760 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1020 06:30:11.910982    6760 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1020 06:30:12.149759    6760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1020 06:30:12.398822    6760 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1020 06:30:12.505638    6760 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1020 06:30:12.615171    6760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1020 06:30:12.854834    6760 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1020 06:30:13.091946    6760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1020 06:30:13.114955    6760 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1020 06:30:13.166042    6760 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1020 06:30:13.174997    6760 start.go:563] Will wait 60s for crictl version
I1020 06:30:13.226327    6760 ssh_runner.go:195] Run: which crictl
I1020 06:30:13.284784    6760 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1020 06:30:13.570563    6760 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1020 06:30:13.606725    6760 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1020 06:30:13.826127    6760 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1020 06:30:13.922460    6760 out.go:252] 🐳  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1020 06:30:13.963693    6760 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1020 06:30:14.193376    6760 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1020 06:30:14.267542    6760 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1020 06:30:14.317315    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1020 06:30:14.408313    6760 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1020 06:30:14.408313    6760 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1020 06:30:14.458412    6760 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1020 06:30:14.506516    6760 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1020 06:30:14.507522    6760 docker.go:621] Images already preloaded, skipping extraction
I1020 06:30:14.547254    6760 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1020 06:30:14.597984    6760 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1020 06:30:14.597984    6760 cache_images.go:85] Images are preloaded, skipping loading
I1020 06:30:14.597984    6760 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1020 06:30:14.597984    6760 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1020 06:30:14.640341    6760 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1020 06:30:15.116382    6760 cni.go:84] Creating CNI manager for ""
I1020 06:30:15.116382    6760 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1020 06:30:15.116382    6760 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1020 06:30:15.116382    6760 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1020 06:30:15.116382    6760 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1020 06:30:15.280125    6760 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1020 06:30:15.311920    6760 binaries.go:44] Found k8s binaries, skipping transfer
I1020 06:30:15.392856    6760 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1020 06:30:15.412565    6760 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1020 06:30:15.454934    6760 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1020 06:30:15.490768    6760 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2209 bytes)
I1020 06:30:15.598609    6760 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1020 06:30:15.671267    6760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1020 06:30:15.892092    6760 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1020 06:30:15.916137    6760 certs.go:68] Setting up C:\Users\ALSAINT\.minikube\profiles\minikube for IP: 192.168.49.2
I1020 06:30:15.916137    6760 certs.go:194] generating shared ca certs ...
I1020 06:30:15.916653    6760 certs.go:226] acquiring lock for ca certs: {Name:mk3eaf030d6a6ab13bbc246df5790121bb388234 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1020 06:30:15.918303    6760 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\ALSAINT\.minikube\ca.key
I1020 06:30:15.934846    6760 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\ALSAINT\.minikube\proxy-client-ca.key
I1020 06:30:15.934846    6760 certs.go:256] generating profile certs ...
I1020 06:30:16.028330    6760 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\ALSAINT\.minikube\profiles\minikube\client.key
I1020 06:30:16.072761    6760 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\ALSAINT\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I1020 06:30:16.112747    6760 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\ALSAINT\.minikube\profiles\minikube\proxy-client.key
I1020 06:30:16.115171    6760 certs.go:484] found cert: C:\Users\ALSAINT\.minikube\certs\ca-key.pem (1679 bytes)
I1020 06:30:16.147362    6760 certs.go:484] found cert: C:\Users\ALSAINT\.minikube\certs\ca.pem (1082 bytes)
I1020 06:30:16.147362    6760 certs.go:484] found cert: C:\Users\ALSAINT\.minikube\certs\cert.pem (1123 bytes)
I1020 06:30:16.147902    6760 certs.go:484] found cert: C:\Users\ALSAINT\.minikube\certs\key.pem (1675 bytes)
I1020 06:30:16.230988    6760 ssh_runner.go:362] scp C:\Users\ALSAINT\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1020 06:30:16.323811    6760 ssh_runner.go:362] scp C:\Users\ALSAINT\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1020 06:30:16.427036    6760 ssh_runner.go:362] scp C:\Users\ALSAINT\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1020 06:30:16.510297    6760 ssh_runner.go:362] scp C:\Users\ALSAINT\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1020 06:30:16.564096    6760 ssh_runner.go:362] scp C:\Users\ALSAINT\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1020 06:30:16.618986    6760 ssh_runner.go:362] scp C:\Users\ALSAINT\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1020 06:30:16.682858    6760 ssh_runner.go:362] scp C:\Users\ALSAINT\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1020 06:30:16.735938    6760 ssh_runner.go:362] scp C:\Users\ALSAINT\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1020 06:30:16.793231    6760 ssh_runner.go:362] scp C:\Users\ALSAINT\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1020 06:30:16.846566    6760 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I1020 06:30:16.943789    6760 ssh_runner.go:195] Run: openssl version
I1020 06:30:17.010502    6760 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1020 06:30:17.088428    6760 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1020 06:30:17.097190    6760 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct 20 04:22 /usr/share/ca-certificates/minikubeCA.pem
I1020 06:30:17.148086    6760 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1020 06:30:17.221250    6760 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1020 06:30:17.283603    6760 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1020 06:30:17.347234    6760 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1020 06:30:17.410620    6760 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1020 06:30:17.490165    6760 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1020 06:30:17.554918    6760 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1020 06:30:17.619653    6760 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1020 06:30:17.684404    6760 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1020 06:30:17.698169    6760 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1020 06:30:17.732755    6760 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1020 06:30:17.839557    6760 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1020 06:30:17.870601    6760 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1020 06:30:17.908714    6760 kubeadm.go:589] restartPrimaryControlPlane start ...
I1020 06:30:18.011840    6760 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1020 06:30:18.031067    6760 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1020 06:30:18.066093    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1020 06:30:18.221203    6760 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:49970"
I1020 06:30:18.503708    6760 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1020 06:30:18.599691    6760 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I1020 06:30:18.667531    6760 kubeadm.go:593] duration metric: took 758.8176ms to restartPrimaryControlPlane
I1020 06:30:18.667531    6760 kubeadm.go:394] duration metric: took 969.3626ms to StartCluster
I1020 06:30:18.667531    6760 settings.go:142] acquiring lock: {Name:mk4e1351236f6c07e49b61936bd56e7df2441639 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1020 06:30:18.667531    6760 settings.go:150] Updating kubeconfig:  C:\Users\ALSAINT\.kube\config
I1020 06:30:18.827090    6760 lock.go:35] WriteFile acquiring C:\Users\ALSAINT\.kube\config: {Name:mk8f68e77044c7edec64a607438c54dca6af30b3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1020 06:30:18.830653    6760 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1020 06:30:18.830874    6760 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1020 06:30:18.830874    6760 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1020 06:30:18.830874    6760 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W1020 06:30:18.830874    6760 addons.go:247] addon storage-provisioner should already be in state true
I1020 06:30:18.830874    6760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1020 06:30:18.831698    6760 host.go:66] Checking if "minikube" exists ...
I1020 06:30:18.831698    6760 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1020 06:30:18.831698    6760 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1020 06:30:18.895417    6760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1020 06:30:18.909933    6760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1020 06:30:18.935215    6760 out.go:179] 🔎  Verifying Kubernetes components...
I1020 06:30:19.233142    6760 addons.go:238] Setting addon default-storageclass=true in "minikube"
W1020 06:30:19.233142    6760 addons.go:247] addon default-storageclass should already be in state true
I1020 06:30:19.233142    6760 host.go:66] Checking if "minikube" exists ...
I1020 06:30:19.290787    6760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1020 06:30:19.317446    6760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1020 06:30:19.396150    6760 out.go:179]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1020 06:30:19.568930    6760 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1020 06:30:19.568930    6760 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1020 06:30:19.572058    6760 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1020 06:30:19.576602    6760 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1020 06:30:19.576602    6760 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1020 06:30:19.613907    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:30:19.619772    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1020 06:30:19.646920    6760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1020 06:30:19.740544    6760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49971 SSHKeyPath:C:\Users\ALSAINT\.minikube\machines\minikube\id_rsa Username:docker}
I1020 06:30:19.756707    6760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49971 SSHKeyPath:C:\Users\ALSAINT\.minikube\machines\minikube\id_rsa Username:docker}
I1020 06:30:19.760321    6760 api_server.go:52] waiting for apiserver process to appear ...
I1020 06:30:19.822720    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:20.037356    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1020 06:30:20.051295    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W1020 06:30:20.206073    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1020 06:30:20.206073    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:20.227250    6760 retry.go:31] will retry after 225.322534ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:20.316722    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:20.467847    6760 retry.go:31] will retry after 233.734633ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:20.508530    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1020 06:30:20.624939    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:20.624939    6760 retry.go:31] will retry after 306.110722ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:20.755420    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1020 06:30:20.818750    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1020 06:30:20.892698    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:20.892698    6760 retry.go:31] will retry after 292.788894ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:20.992729    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1020 06:30:21.087937    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:21.087937    6760 retry.go:31] will retry after 798.320985ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:21.237931    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1020 06:30:21.322495    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1020 06:30:21.378102    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:21.378102    6760 retry.go:31] will retry after 333.35146ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:21.766465    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1020 06:30:21.839409    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:21.973733    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1020 06:30:21.985814    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:21.985814    6760 retry.go:31] will retry after 1.026058965s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1020 06:30:22.133568    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:22.133568    6760 retry.go:31] will retry after 743.139287ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:22.330041    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:22.823206    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:22.967730    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1020 06:30:23.093038    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1020 06:30:23.206965    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:23.206965    6760 retry.go:31] will retry after 1.445010737s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1020 06:30:23.346055    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:23.346483    6760 retry.go:31] will retry after 848.530455ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:23.356986    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:23.828889    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:24.267945    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1020 06:30:24.348970    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1020 06:30:24.421284    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:24.421284    6760 retry.go:31] will retry after 1.423246415s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:24.842229    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1020 06:30:24.880238    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1020 06:30:24.982350    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:24.982350    6760 retry.go:31] will retry after 2.006177149s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:25.312950    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:25.832313    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:25.915597    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1020 06:30:26.043581    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:26.043581    6760 retry.go:31] will retry after 4.211613391s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:26.338153    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:26.826771    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:27.055577    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1020 06:30:27.199557    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:27.199557    6760 retry.go:31] will retry after 4.254517936s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:27.328926    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:27.809481    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:28.308742    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:28.867468    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:29.326888    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:29.876145    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:30.323904    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1020 06:30:30.333892    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1020 06:30:30.546119    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:30.546119    6760 retry.go:31] will retry after 5.005356257s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:30.812568    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:31.316875    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:31.526607    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1020 06:30:31.637964    6760 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:31.637964    6760 retry.go:31] will retry after 4.268263358s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1020 06:30:31.816054    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:32.334286    6760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1020 06:30:32.381557    6760 api_server.go:72] duration metric: took 13.550904s to wait for apiserver process to appear ...
I1020 06:30:32.381557    6760 api_server.go:88] waiting for apiserver healthz status ...
I1020 06:30:32.382086    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:32.387263    6760 api_server.go:269] stopped: https://127.0.0.1:49970/healthz: Get "https://127.0.0.1:49970/healthz": EOF
I1020 06:30:32.882600    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:35.672291    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1020 06:30:35.978028    6760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1020 06:30:37.760419    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1020 06:30:37.760419    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1020 06:30:37.760419    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:37.821349    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W1020 06:30:37.821349    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I1020 06:30:37.882970    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:38.030297    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/start-kubernetes-service-cidr-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:38.030297    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/start-kubernetes-service-cidr-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:38.382817    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:38.439718    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:38.439718    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:38.725628    6760 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (3.0533373s)
I1020 06:30:38.882334    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:38.919361    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:38.919361    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:39.382701    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:39.400113    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:39.400113    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:39.882671    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:39.890668    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:39.890668    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:40.382897    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:40.389896    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:40.389896    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:40.882338    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:41.115101    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:41.115101    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:41.382603    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:41.440822    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:41.440822    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:41.772637    6760 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (5.7946086s)
I1020 06:30:41.838620    6760 out.go:179] 🌟  Enabled addons: default-storageclass, storage-provisioner
I1020 06:30:41.882830    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:41.888896    6760 addons.go:514] duration metric: took 23.0575047s for enable addons: enabled=[default-storageclass storage-provisioner]
I1020 06:30:41.893711    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:41.893711    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:42.382941    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:42.391943    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:42.391943    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:42.882985    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:42.890991    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:42.890991    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:43.383285    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:43.391284    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:43.391284    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:43.882373    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:43.944249    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:43.944249    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:44.382698    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:44.391263    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:44.391263    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:44.883318    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:44.899296    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:44.899296    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:45.382662    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:45.390665    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:45.390665    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:45.882392    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:45.893331    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:45.893331    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:46.382535    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:46.459105    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:46.459105    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:46.882163    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:46.892470    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:46.892470    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:47.382512    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:47.401655    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:47.401655    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:47.882246    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:47.950482    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:47.950482    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:48.382872    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:48.392812    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1020 06:30:48.392812    6760 api_server.go:103] status: https://127.0.0.1:49970/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1020 06:30:48.883252    6760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49970/healthz ...
I1020 06:30:48.895776    6760 api_server.go:279] https://127.0.0.1:49970/healthz returned 200:
ok
I1020 06:30:49.111915    6760 api_server.go:141] control plane version: v1.34.0
I1020 06:30:49.111915    6760 api_server.go:131] duration metric: took 16.7303581s to wait for apiserver health ...
I1020 06:30:49.111915    6760 system_pods.go:43] waiting for kube-system pods to appear ...
I1020 06:30:49.676604    6760 system_pods.go:59] 7 kube-system pods found
I1020 06:30:49.676604    6760 system_pods.go:61] "coredns-66bc5c9577-nhrn7" [eea835d9-a82e-45f1-92ee-d99de871d32a] Running
I1020 06:30:49.676604    6760 system_pods.go:61] "etcd-minikube" [ce8004bc-9c80-4f0d-b490-e32d8b636a0c] Running
I1020 06:30:49.676604    6760 system_pods.go:61] "kube-apiserver-minikube" [4e630c8c-dcd2-48e1-80b2-98681877a837] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1020 06:30:49.676604    6760 system_pods.go:61] "kube-controller-manager-minikube" [ddad29e8-6200-427a-9299-51d1af431800] Running
I1020 06:30:49.676604    6760 system_pods.go:61] "kube-proxy-2snft" [0d495b3a-031f-44cf-8e2a-0d49e8c2dd03] Running
I1020 06:30:49.676604    6760 system_pods.go:61] "kube-scheduler-minikube" [c6c9a75c-f97c-496a-b225-bdf272078feb] Running
I1020 06:30:49.676604    6760 system_pods.go:61] "storage-provisioner" [0c83f630-643b-414c-b6bb-294a9e466f53] Running
I1020 06:30:49.676604    6760 system_pods.go:74] duration metric: took 564.6885ms to wait for pod list to return data ...
I1020 06:30:49.676604    6760 kubeadm.go:578] duration metric: took 30.8459506s to wait for: map[apiserver:true system_pods:true]
I1020 06:30:49.676604    6760 node_conditions.go:102] verifying NodePressure condition ...
I1020 06:30:49.817783    6760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1020 06:30:49.817783    6760 node_conditions.go:123] node cpu capacity is 4
I1020 06:30:49.817783    6760 node_conditions.go:105] duration metric: took 141.1789ms to run NodePressure ...
I1020 06:30:49.817783    6760 start.go:241] waiting for startup goroutines ...
I1020 06:30:49.817783    6760 start.go:246] waiting for cluster config update ...
I1020 06:30:49.817783    6760 start.go:255] writing updated cluster config ...
I1020 06:30:49.891684    6760 ssh_runner.go:195] Run: rm -f paused
I1020 06:30:51.223559    6760 start.go:617] kubectl: 1.34.1, cluster: 1.34.0 (minor skew: 0)
I1020 06:30:51.331606    6760 out.go:179] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Oct 20 04:30:09 minikube dockerd[4607]: time="2025-10-20T04:30:09.072197680Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 3e670371da3e10644b9f1473179d6b8e1cc89e5cf480270d2d8cecb95794ee31 d7fda664a6691dfc7f9c5cd8861291407b26fc1532a761cccc137947e590791b], retrying...."
Oct 20 04:30:10 minikube dockerd[4607]: time="2025-10-20T04:30:10.047785683Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 20ecbea42cbe9bd9eeb45f775f017242e3585ba1a88b2f64825b366541569cbe], retrying...."
Oct 20 04:30:10 minikube dockerd[4607]: time="2025-10-20T04:30:10.864135959Z" level=warning msg="error locating sandbox id f4e5749f2b30506bd1c90a3fd5e133543464fa4fceb983906ca23aab47cf28bd: sandbox f4e5749f2b30506bd1c90a3fd5e133543464fa4fceb983906ca23aab47cf28bd not found"
Oct 20 04:30:10 minikube dockerd[4607]: time="2025-10-20T04:30:10.864222266Z" level=warning msg="error locating sandbox id 6130a999023e28114355ad03548491b259470c5a4440a9244c12d60a8edeaf20: sandbox 6130a999023e28114355ad03548491b259470c5a4440a9244c12d60a8edeaf20 not found"
Oct 20 04:30:10 minikube dockerd[4607]: time="2025-10-20T04:30:10.864249368Z" level=warning msg="error locating sandbox id b782bedb531d0ded399e157dd6768aeb44cde545902db9f514b3cc01883f1b71: sandbox b782bedb531d0ded399e157dd6768aeb44cde545902db9f514b3cc01883f1b71 not found"
Oct 20 04:30:10 minikube dockerd[4607]: time="2025-10-20T04:30:10.864269369Z" level=warning msg="error locating sandbox id ee1ffdda890b0cbbeba5dec6db0d0cb094c7bef7ed354f2e576083c6bd094fba: sandbox ee1ffdda890b0cbbeba5dec6db0d0cb094c7bef7ed354f2e576083c6bd094fba not found"
Oct 20 04:30:10 minikube dockerd[4607]: time="2025-10-20T04:30:10.864283771Z" level=warning msg="error locating sandbox id 06fa4dc4b62d680600d18f1ada1974760e768c02cb553008719d5b690cb8fdee: sandbox 06fa4dc4b62d680600d18f1ada1974760e768c02cb553008719d5b690cb8fdee not found"
Oct 20 04:30:10 minikube dockerd[4607]: time="2025-10-20T04:30:10.864299672Z" level=warning msg="error locating sandbox id 1812e540bf0d45de0e2bf2292213e3c7926fd330d88a63685e061c4dca0b5678: sandbox 1812e540bf0d45de0e2bf2292213e3c7926fd330d88a63685e061c4dca0b5678 not found"
Oct 20 04:30:10 minikube dockerd[4607]: time="2025-10-20T04:30:10.864314173Z" level=warning msg="error locating sandbox id 1d664be6f8fe101014de584f5668c5503d8c43bbd63f574d461e196f722df5b6: sandbox 1d664be6f8fe101014de584f5668c5503d8c43bbd63f574d461e196f722df5b6 not found"
Oct 20 04:30:10 minikube dockerd[4607]: time="2025-10-20T04:30:10.864582194Z" level=info msg="Loading containers: done."
Oct 20 04:30:10 minikube dockerd[4607]: time="2025-10-20T04:30:10.943223746Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Oct 20 04:30:10 minikube dockerd[4607]: time="2025-10-20T04:30:10.943364158Z" level=info msg="Initializing buildkit"
Oct 20 04:30:10 minikube dockerd[4607]: time="2025-10-20T04:30:10.992212641Z" level=info msg="Completed buildkit initialization"
Oct 20 04:30:11 minikube dockerd[4607]: time="2025-10-20T04:30:11.006588684Z" level=info msg="Daemon has completed initialization"
Oct 20 04:30:11 minikube dockerd[4607]: time="2025-10-20T04:30:11.007743776Z" level=info msg="API listen on /run/docker.sock"
Oct 20 04:30:11 minikube dockerd[4607]: time="2025-10-20T04:30:11.008178511Z" level=info msg="API listen on [::]:2376"
Oct 20 04:30:11 minikube dockerd[4607]: time="2025-10-20T04:30:11.008185411Z" level=info msg="API listen on /var/run/docker.sock"
Oct 20 04:30:11 minikube systemd[1]: Started Docker Application Container Engine.
Oct 20 04:30:11 minikube cri-dockerd[1168]: time="2025-10-20T04:30:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-nhrn7_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"98d67d20dce5525b504bf9b08e43e875645acb9faa262ab0ce3f5aa77c8413da\""
Oct 20 04:30:11 minikube systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
Oct 20 04:30:11 minikube systemd[1]: cri-docker.service: Deactivated successfully.
Oct 20 04:30:11 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Oct 20 04:30:11 minikube systemd[1]: cri-docker.service: Consumed 6.988s CPU time.
Oct 20 04:30:12 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Oct 20 04:30:12 minikube cri-dockerd[5409]: time="2025-10-20T04:30:12Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Oct 20 04:30:12 minikube cri-dockerd[5409]: time="2025-10-20T04:30:12Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Oct 20 04:30:12 minikube cri-dockerd[5409]: time="2025-10-20T04:30:12Z" level=info msg="Start docker client with request timeout 0s"
Oct 20 04:30:12 minikube cri-dockerd[5409]: time="2025-10-20T04:30:12Z" level=info msg="Hairpin mode is set to hairpin-veth"
Oct 20 04:30:12 minikube cri-dockerd[5409]: time="2025-10-20T04:30:12Z" level=info msg="Loaded network plugin cni"
Oct 20 04:30:12 minikube cri-dockerd[5409]: time="2025-10-20T04:30:12Z" level=info msg="Docker cri networking managed by network plugin cni"
Oct 20 04:30:12 minikube cri-dockerd[5409]: time="2025-10-20T04:30:12Z" level=info msg="Setting cgroupDriver cgroupfs"
Oct 20 04:30:12 minikube cri-dockerd[5409]: time="2025-10-20T04:30:12Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Oct 20 04:30:12 minikube cri-dockerd[5409]: time="2025-10-20T04:30:12Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Oct 20 04:30:12 minikube cri-dockerd[5409]: time="2025-10-20T04:30:12Z" level=info msg="Start cri-dockerd grpc backend"
Oct 20 04:30:12 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Oct 20 04:30:12 minikube systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
Oct 20 04:30:12 minikube systemd[1]: cri-docker.service: Deactivated successfully.
Oct 20 04:30:12 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Oct 20 04:30:12 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Oct 20 04:30:12 minikube cri-dockerd[5508]: time="2025-10-20T04:30:12Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Oct 20 04:30:12 minikube cri-dockerd[5508]: time="2025-10-20T04:30:12Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Oct 20 04:30:12 minikube cri-dockerd[5508]: time="2025-10-20T04:30:12Z" level=info msg="Start docker client with request timeout 0s"
Oct 20 04:30:12 minikube cri-dockerd[5508]: time="2025-10-20T04:30:12Z" level=info msg="Hairpin mode is set to hairpin-veth"
Oct 20 04:30:13 minikube cri-dockerd[5508]: time="2025-10-20T04:30:13Z" level=info msg="Loaded network plugin cni"
Oct 20 04:30:13 minikube cri-dockerd[5508]: time="2025-10-20T04:30:13Z" level=info msg="Docker cri networking managed by network plugin cni"
Oct 20 04:30:13 minikube cri-dockerd[5508]: time="2025-10-20T04:30:13Z" level=info msg="Setting cgroupDriver cgroupfs"
Oct 20 04:30:13 minikube cri-dockerd[5508]: time="2025-10-20T04:30:13Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Oct 20 04:30:13 minikube cri-dockerd[5508]: time="2025-10-20T04:30:13Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Oct 20 04:30:13 minikube cri-dockerd[5508]: time="2025-10-20T04:30:13Z" level=info msg="Start cri-dockerd grpc backend"
Oct 20 04:30:13 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Oct 20 04:30:13 minikube cri-dockerd[5508]: time="2025-10-20T04:30:13Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c135e29c2233c97a837a56cbe353c252b37ba5df6ead734a9ba71cd8b47dd729"
Oct 20 04:30:14 minikube cri-dockerd[5508]: time="2025-10-20T04:30:14Z" level=info msg="Successfully removed conflicting container: c135e29c2233c97a837a56cbe353c252b37ba5df6ead734a9ba71cd8b47dd729"
Oct 20 04:30:21 minikube cri-dockerd[5508]: time="2025-10-20T04:30:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a7b6f25138d571e116dfd23072c4d126e871174910445e743d19ef7b1badbb7b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 20 04:30:22 minikube cri-dockerd[5508]: time="2025-10-20T04:30:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8542e49c2001289366dfd174c2e0ef16e05a70b41ecad2526d99318a9aaeb1b1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 20 04:30:25 minikube cri-dockerd[5508]: time="2025-10-20T04:30:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cf345086ed5efa7f4033b50c7c6c0f6cb97de270688fdd7ac46aa8395fda7953/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 20 04:30:25 minikube cri-dockerd[5508]: time="2025-10-20T04:30:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3528f2fa14364860797133ebd7cafc7c409b9391637af73277162e08f3e9f869/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 20 04:30:25 minikube cri-dockerd[5508]: time="2025-10-20T04:30:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/95778fc8ea1b993270ac055b9ca686aae9418ab0f77968adb5b37a1a0ec094ba/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 20 04:30:26 minikube cri-dockerd[5508]: time="2025-10-20T04:30:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8593c8231d5224111266a338e21eda46c0b708fcc845e83da4ffc70375ac0af3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 20 04:30:31 minikube cri-dockerd[5508]: time="2025-10-20T04:30:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0ab709de8ee2cf3c051de59108528bdb72f4aa9ad0bdf73963874fca1c8b3cf2/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 20 04:30:47 minikube dockerd[4607]: time="2025-10-20T04:30:47.987884306Z" level=info msg="ignoring event" container=79e298e73d41c1361e2f85dba8ee64104fccd4fb8351c8be5258dcd102598fb9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
0e4ebd7545e19       a0af72f2ec6d6       About a minute ago   Running             kube-controller-manager   3                   8593c8231d522       kube-controller-manager-minikube
57fb1ab9e5ddd       6e38f40d628db       2 minutes ago        Running             storage-provisioner       2                   0ab709de8ee2c       storage-provisioner
79e298e73d41c       a0af72f2ec6d6       2 minutes ago        Exited              kube-controller-manager   2                   8593c8231d522       kube-controller-manager-minikube
fbebf127e9aa3       df0860106674d       2 minutes ago        Running             kube-proxy                1                   3528f2fa14364       kube-proxy-2snft
57a0acec854a8       46169d968e920       2 minutes ago        Running             kube-scheduler            1                   95778fc8ea1b9       kube-scheduler-minikube
d6ea0b14b7a6d       90550c43ad2bc       2 minutes ago        Running             kube-apiserver            1                   cf345086ed5ef       kube-apiserver-minikube
01dcfed67b96b       52546a367cc9e       2 minutes ago        Running             coredns                   1                   8542e49c20012       coredns-66bc5c9577-nhrn7
99f1803d218dd       5f1f5298c888d       2 minutes ago        Running             etcd                      1                   a7b6f25138d57       etcd-minikube
ab59f9a8ea601       6e38f40d628db       7 minutes ago        Exited              storage-provisioner       1                   2f81086d2584b       storage-provisioner
f1d6920195bc6       52546a367cc9e       8 minutes ago        Exited              coredns                   0                   98d67d20dce55       coredns-66bc5c9577-nhrn7
b12876f6c9ec9       df0860106674d       8 minutes ago        Exited              kube-proxy                0                   31d51fae4a1c1       kube-proxy-2snft
2696b2dbacf72       46169d968e920       9 minutes ago        Exited              kube-scheduler            0                   56899a8afcd2a       kube-scheduler-minikube
fdb54ad7aea77       90550c43ad2bc       9 minutes ago        Exited              kube-apiserver            0                   1351b3fa3da7c       kube-apiserver-minikube
64afef40464b2       5f1f5298c888d       9 minutes ago        Exited              etcd                      0                   2a040d49de0fe       etcd-minikube


==> coredns [01dcfed67b96] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:41956 - 32564 "HINFO IN 913770612944867188.3761702016805468660. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.187764637s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> coredns [f1d6920195bc] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:42922 - 16127 "HINFO IN 1262822627773519238.510959634960845174. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.112876007s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_10_20T06_23_52_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 20 Oct 2025 04:23:23 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 20 Oct 2025 04:32:42 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 20 Oct 2025 04:30:41 +0000   Mon, 20 Oct 2025 04:23:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 20 Oct 2025 04:30:41 +0000   Mon, 20 Oct 2025 04:23:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 20 Oct 2025 04:30:41 +0000   Mon, 20 Oct 2025 04:23:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 20 Oct 2025 04:30:41 +0000   Mon, 20 Oct 2025 04:23:24 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8072432Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8072432Ki
  pods:               110
System Info:
  Machine ID:                 57b62f76bf294c2693d8d2f2533c08d9
  System UUID:                57b62f76bf294c2693d8d2f2533c08d9
  Boot ID:                    09048dea-4aba-45d3-9d2e-9d20dc46ae00
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (7 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-66bc5c9577-nhrn7            100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     8m43s
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         9m17s
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         9m23s
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         9m23s
  kube-system                 kube-proxy-2snft                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m44s
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         9m24s
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m50s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 8m19s                  kube-proxy       
  Normal   Starting                 2m9s                   kube-proxy       
  Normal   NodeHasNoDiskPressure    9m45s (x8 over 9m45s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   Starting                 9m45s                  kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  9m45s (x8 over 9m45s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasSufficientPID     9m45s (x7 over 9m45s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  9m45s                  kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                 8m59s                  kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced  8m59s                  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  8m59s                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    8m59s                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     8m59s                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode           8m45s                  node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  ContainerGCFailed        2m58s                  kubelet          rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
  Normal   NodeNotReady             2m37s                  kubelet          Node minikube status is now: NodeNotReady
  Normal   RegisteredNode           94s                    node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Oct20 03:30] PCI: Fatal: No config space access function found
[  +0.040630] PCI: System does not support PCI
[  +0.126656] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.125928] Warning: unable to open an initial console.
[  +0.003201] WSL (1 - ) ERROR: InitializeLogging:1628: open(/dev/hvc1) failed 2
[  +0.000005] WSL (1 - ) ERROR: main:3921: /init was started without /dev/console
[  +3.928438] hv_balloon: Cold memory discard hypercall failed with status 1900000005
[  +0.000841] hv_balloon: Underlying Hyper-V does not support order less than 9. Hypercall failed
[  +0.000674] hv_balloon: Defaulting to page_reporting_order 9
[ +10.473118] WSL (188) ERROR: CheckConnection: getaddrinfo() failed: -5
[ +14.297814] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2058: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.212528] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Budapest not found. Is the tzdata package installed?
[  +6.822844] pulseaudio[236]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[Oct20 03:31] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.022250] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000617] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001018] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000888] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002506] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000643] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000551] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000767] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[ +25.298773] netlink: 'init': attribute type 4 has an invalid length.
[  +0.652524] virtiofs: Unknown parameter 'negative_dentry_timeout'
[Oct20 03:57] WSL (188) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct20 03:58] WSL (188) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct20 04:29] hrtimer: interrupt took 3298957 ns


==> etcd [64afef40464b] <==
{"level":"info","ts":"2025-10-20T04:29:25.396431Z","caller":"traceutil/trace.go:172","msg":"trace[1525948357] transaction","detail":"{read_only:false; response_revision:695; number_of_response:1; }","duration":"142.954828ms","start":"2025-10-20T04:29:25.253449Z","end":"2025-10-20T04:29:25.396404Z","steps":["trace[1525948357] 'process raft request'  (duration: 142.815417ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:29:25.731806Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"206.363764ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/resourceclaims\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:29:25.731869Z","caller":"traceutil/trace.go:172","msg":"trace[263531985] range","detail":"{range_begin:/registry/resourceclaims; range_end:; response_count:0; response_revision:695; }","duration":"206.44017ms","start":"2025-10-20T04:29:25.525418Z","end":"2025-10-20T04:29:25.731859Z","steps":["trace[263531985] 'range keys from in-memory index tree'  (duration: 206.264156ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:29:27.728809Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"171.866979ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/podtemplates\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:29:27.728889Z","caller":"traceutil/trace.go:172","msg":"trace[2003648161] range","detail":"{range_begin:/registry/podtemplates; range_end:; response_count:0; response_revision:695; }","duration":"171.954586ms","start":"2025-10-20T04:29:27.556922Z","end":"2025-10-20T04:29:27.728877Z","steps":["trace[2003648161] 'range keys from in-memory index tree'  (duration: 164.16438ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:29:27.728981Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"164.279588ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128040751674730403 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:689 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128040751674730400 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-10-20T04:29:27.729102Z","caller":"traceutil/trace.go:172","msg":"trace[2082649530] transaction","detail":"{read_only:false; response_revision:697; number_of_response:1; }","duration":"240.813745ms","start":"2025-10-20T04:29:27.488281Z","end":"2025-10-20T04:29:27.729095Z","steps":["trace[2082649530] 'process raft request'  (duration: 240.763442ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:29:27.729147Z","caller":"traceutil/trace.go:172","msg":"trace[1922858909] linearizableReadLoop","detail":"{readStateIndex:781; appliedIndex:780; }","duration":"163.671241ms","start":"2025-10-20T04:29:27.565468Z","end":"2025-10-20T04:29:27.729140Z","steps":["trace[1922858909] 'read index received'  (duration: 114.103982ms)","trace[1922858909] 'applied index is now lower than readState.Index'  (duration: 49.566459ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-20T04:29:27.729190Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"163.719044ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:29:27.729192Z","caller":"traceutil/trace.go:172","msg":"trace[1282433895] transaction","detail":"{read_only:false; response_revision:696; number_of_response:1; }","duration":"243.631365ms","start":"2025-10-20T04:29:27.485549Z","end":"2025-10-20T04:29:27.729180Z","steps":["trace[1282433895] 'process raft request'  (duration: 79.104458ms)","trace[1282433895] 'compare'  (duration: 164.218483ms)"],"step_count":2}
{"level":"info","ts":"2025-10-20T04:29:27.729209Z","caller":"traceutil/trace.go:172","msg":"trace[439057068] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:697; }","duration":"163.739646ms","start":"2025-10-20T04:29:27.565464Z","end":"2025-10-20T04:29:27.729204Z","steps":["trace[439057068] 'agreement among raft nodes before linearized reading'  (duration: 163.701043ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:29:32.233459Z","caller":"traceutil/trace.go:172","msg":"trace[1051799377] transaction","detail":"{read_only:false; response_revision:701; number_of_response:1; }","duration":"111.944109ms","start":"2025-10-20T04:29:32.121502Z","end":"2025-10-20T04:29:32.233446Z","steps":["trace[1051799377] 'process raft request'  (duration: 111.591282ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:29:32.671288Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"140.694246ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128040751674730432 > lease_revoke:<id:70cc99ffdb78a768>","response":"size:29"}
{"level":"info","ts":"2025-10-20T04:29:32.671375Z","caller":"traceutil/trace.go:172","msg":"trace[87059558] linearizableReadLoop","detail":"{readStateIndex:787; appliedIndex:786; }","duration":"110.579703ms","start":"2025-10-20T04:29:32.560785Z","end":"2025-10-20T04:29:32.671365Z","steps":["trace[87059558] 'read index received'  (duration: 37.003µs)","trace[87059558] 'applied index is now lower than readState.Index'  (duration: 110.5417ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-20T04:29:32.671424Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"110.632907ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:29:32.671441Z","caller":"traceutil/trace.go:172","msg":"trace[1313533803] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:701; }","duration":"110.652309ms","start":"2025-10-20T04:29:32.560782Z","end":"2025-10-20T04:29:32.671434Z","steps":["trace[1313533803] 'agreement among raft nodes before linearized reading'  (duration: 110.612906ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:29:34.047897Z","caller":"traceutil/trace.go:172","msg":"trace[1723174393] transaction","detail":"{read_only:false; response_revision:702; number_of_response:1; }","duration":"101.387688ms","start":"2025-10-20T04:29:33.946494Z","end":"2025-10-20T04:29:34.047882Z","steps":["trace[1723174393] 'process raft request'  (duration: 101.246077ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:29:34.383672Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"172.615329ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:29:34.383754Z","caller":"traceutil/trace.go:172","msg":"trace[113093832] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:702; }","duration":"172.710437ms","start":"2025-10-20T04:29:34.211032Z","end":"2025-10-20T04:29:34.383742Z","steps":["trace[113093832] 'range keys from in-memory index tree'  (duration: 172.575626ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:29:36.176599Z","caller":"traceutil/trace.go:172","msg":"trace[1532571141] transaction","detail":"{read_only:false; response_revision:703; number_of_response:1; }","duration":"120.972012ms","start":"2025-10-20T04:29:36.055614Z","end":"2025-10-20T04:29:36.176586Z","steps":["trace[1532571141] 'process raft request'  (duration: 120.860403ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:29:40.341868Z","caller":"traceutil/trace.go:172","msg":"trace[74298767] transaction","detail":"{read_only:false; response_revision:707; number_of_response:1; }","duration":"122.318376ms","start":"2025-10-20T04:29:40.219533Z","end":"2025-10-20T04:29:40.341851Z","steps":["trace[74298767] 'process raft request'  (duration: 122.191566ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:29:48.313131Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"102.32474ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:29:48.313228Z","caller":"traceutil/trace.go:172","msg":"trace[157767361] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:713; }","duration":"102.418347ms","start":"2025-10-20T04:29:48.210776Z","end":"2025-10-20T04:29:48.313194Z","steps":["trace[157767361] 'range keys from in-memory index tree'  (duration: 102.269335ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:29:50.367906Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"157.109644ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:29:50.367997Z","caller":"traceutil/trace.go:172","msg":"trace[912455164] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:715; }","duration":"157.196551ms","start":"2025-10-20T04:29:50.210774Z","end":"2025-10-20T04:29:50.367970Z","steps":["trace[912455164] 'range keys from in-memory index tree'  (duration: 157.071841ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:29:50.368071Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"169.405009ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128040751674730538 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/minikube.187018ae0a3eac31\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/minikube.187018ae0a3eac31\" value_size:648 lease:8128040751674730536 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2025-10-20T04:29:50.368165Z","caller":"traceutil/trace.go:172","msg":"trace[4497413] transaction","detail":"{read_only:false; response_revision:716; number_of_response:1; }","duration":"230.832936ms","start":"2025-10-20T04:29:50.137321Z","end":"2025-10-20T04:29:50.368154Z","steps":["trace[4497413] 'process raft request'  (duration: 61.308017ms)","trace[4497413] 'compare'  (duration: 169.300801ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-20T04:29:52.802179Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"237.947995ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128040751674730550 > lease_revoke:<id:70cc99ffdb78a7d7>","response":"size:29"}
{"level":"warn","ts":"2025-10-20T04:29:52.802390Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"240.749815ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:29:52.802428Z","caller":"traceutil/trace.go:172","msg":"trace[1524075806] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:717; }","duration":"240.791819ms","start":"2025-10-20T04:29:52.561625Z","end":"2025-10-20T04:29:52.802417Z","steps":["trace[1524075806] 'range keys from in-memory index tree'  (duration: 238.105808ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:29:52.802476Z","caller":"traceutil/trace.go:172","msg":"trace[652638510] linearizableReadLoop","detail":"{readStateIndex:808; appliedIndex:807; }","duration":"124.792704ms","start":"2025-10-20T04:29:52.677670Z","end":"2025-10-20T04:29:52.802462Z","steps":["trace[652638510] 'read index received'  (duration: 34.502µs)","trace[652638510] 'applied index is now lower than readState.Index'  (duration: 124.757302ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-20T04:29:52.802544Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"124.868211ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deviceclasses\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:29:52.802616Z","caller":"traceutil/trace.go:172","msg":"trace[100560791] range","detail":"{range_begin:/registry/deviceclasses; range_end:; response_count:0; response_revision:717; }","duration":"124.893013ms","start":"2025-10-20T04:29:52.677665Z","end":"2025-10-20T04:29:52.802558Z","steps":["trace[100560791] 'agreement among raft nodes before linearized reading'  (duration: 124.851109ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:29:52.978628Z","caller":"traceutil/trace.go:172","msg":"trace[420479602] linearizableReadLoop","detail":"{readStateIndex:808; appliedIndex:808; }","duration":"176.113237ms","start":"2025-10-20T04:29:52.802497Z","end":"2025-10-20T04:29:52.978611Z","steps":["trace[420479602] 'read index received'  (duration: 176.107336ms)","trace[420479602] 'applied index is now lower than readState.Index'  (duration: 5.301µs)"],"step_count":2}
{"level":"info","ts":"2025-10-20T04:29:52.978918Z","caller":"traceutil/trace.go:172","msg":"trace[1640191937] transaction","detail":"{read_only:false; response_revision:718; number_of_response:1; }","duration":"282.388087ms","start":"2025-10-20T04:29:52.696517Z","end":"2025-10-20T04:29:52.978905Z","steps":["trace[1640191937] 'process raft request'  (duration: 282.188471ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:29:52.979292Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"280.747557ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2025-10-20T04:29:52.979354Z","caller":"traceutil/trace.go:172","msg":"trace[5506646] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:717; }","duration":"280.821763ms","start":"2025-10-20T04:29:52.698522Z","end":"2025-10-20T04:29:52.979344Z","steps":["trace[5506646] 'agreement among raft nodes before linearized reading'  (duration: 280.171112ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:29:53.196699Z","caller":"traceutil/trace.go:172","msg":"trace[1746114788] transaction","detail":"{read_only:false; response_revision:719; number_of_response:1; }","duration":"213.524776ms","start":"2025-10-20T04:29:52.983156Z","end":"2025-10-20T04:29:53.196681Z","steps":["trace[1746114788] 'process raft request'  (duration: 122.484623ms)","trace[1746114788] 'compare'  (duration: 90.610119ms)"],"step_count":2}
{"level":"info","ts":"2025-10-20T04:29:54.706050Z","caller":"osutil/interrupt_unix.go:65","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-10-20T04:29:54.706156Z","caller":"embed/etcd.go:426","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"error","ts":"2025-10-20T04:29:54.706343Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}
{"level":"warn","ts":"2025-10-20T04:29:54.822172Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"119.424153ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-10-20T04:29:54.822103Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"120.942973ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2025-10-20T04:29:54.822236Z","caller":"traceutil/trace.go:172","msg":"trace[510808335] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:720; }","duration":"119.471257ms","start":"2025-10-20T04:29:54.702738Z","end":"2025-10-20T04:29:54.822209Z","steps":["trace[510808335] 'range keys from in-memory index tree'  (duration: 119.313443ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:29:54.822247Z","caller":"traceutil/trace.go:172","msg":"trace[1220622574] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:720; }","duration":"121.096585ms","start":"2025-10-20T04:29:54.701141Z","end":"2025-10-20T04:29:54.822237Z","steps":["trace[1220622574] 'range keys from in-memory index tree'  (duration: 120.816362ms)"],"step_count":1}
{"level":"error","ts":"2025-10-20T04:30:01.707399Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}
{"level":"error","ts":"2025-10-20T04:30:01.707503Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 127.0.0.1:2381: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2025-10-20T04:30:01.707532Z","caller":"etcdserver/server.go:1281","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-10-20T04:30:01.707610Z","caller":"etcdserver/server.go:2342","msg":"server has stopped; stopping storage version's monitor"}
{"level":"warn","ts":"2025-10-20T04:30:01.707562Z","caller":"embed/serve.go:245","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"info","ts":"2025-10-20T04:30:01.707628Z","caller":"etcdserver/server.go:2319","msg":"server has stopped; stopping cluster version's monitor"}
{"level":"warn","ts":"2025-10-20T04:30:01.707631Z","caller":"embed/serve.go:247","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"error","ts":"2025-10-20T04:30:01.707654Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 127.0.0.1:2379: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"warn","ts":"2025-10-20T04:30:01.707703Z","caller":"embed/serve.go:245","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-10-20T04:30:01.707729Z","caller":"embed/serve.go:247","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"error","ts":"2025-10-20T04:30:01.707739Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 192.168.49.2:2379: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2025-10-20T04:30:01.836751Z","caller":"embed/etcd.go:621","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"error","ts":"2025-10-20T04:30:01.836936Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 192.168.49.2:2380: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2025-10-20T04:30:01.837014Z","caller":"embed/etcd.go:626","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-10-20T04:30:01.837041Z","caller":"embed/etcd.go:428","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [99f1803d218d] <==
{"level":"warn","ts":"2025-10-20T04:32:22.295062Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"173.405599ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:32:22.295082Z","caller":"traceutil/trace.go:172","msg":"trace[1734075786] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:915; }","duration":"173.444202ms","start":"2025-10-20T04:32:22.121631Z","end":"2025-10-20T04:32:22.295075Z","steps":["trace[1734075786] 'agreement among raft nodes before linearized reading'  (duration: 173.379997ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:32:22.295292Z","caller":"traceutil/trace.go:172","msg":"trace[1284219072] transaction","detail":"{read_only:false; response_revision:916; number_of_response:1; }","duration":"253.625082ms","start":"2025-10-20T04:32:22.041655Z","end":"2025-10-20T04:32:22.295280Z","steps":["trace[1284219072] 'process raft request'  (duration: 253.381063ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:32:25.184547Z","caller":"traceutil/trace.go:172","msg":"trace[981124575] transaction","detail":"{read_only:false; response_revision:919; number_of_response:1; }","duration":"117.107319ms","start":"2025-10-20T04:32:25.067424Z","end":"2025-10-20T04:32:25.184531Z","steps":["trace[981124575] 'process raft request'  (duration: 116.937606ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:32:27.318459Z","caller":"traceutil/trace.go:172","msg":"trace[1142182821] transaction","detail":"{read_only:false; response_revision:920; number_of_response:1; }","duration":"126.478959ms","start":"2025-10-20T04:32:27.191962Z","end":"2025-10-20T04:32:27.318441Z","steps":["trace[1142182821] 'process raft request'  (duration: 126.239541ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:32:28.926342Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"198.100485ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128040751784692598 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:913 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128040751784692596 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-10-20T04:32:28.926472Z","caller":"traceutil/trace.go:172","msg":"trace[312259706] linearizableReadLoop","detail":"{readStateIndex:1037; appliedIndex:1036; }","duration":"100.386146ms","start":"2025-10-20T04:32:28.827718Z","end":"2025-10-20T04:32:28.926462Z","steps":["trace[312259706] 'read index received'  (duration: 29.003µs)","trace[312259706] 'applied index is now lower than readState.Index'  (duration: 100.356243ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-20T04:32:28.926539Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"100.485053ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:32:28.926604Z","caller":"traceutil/trace.go:172","msg":"trace[1016223777] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:921; }","duration":"100.552058ms","start":"2025-10-20T04:32:28.827685Z","end":"2025-10-20T04:32:28.926594Z","steps":["trace[1016223777] 'agreement among raft nodes before linearized reading'  (duration: 100.456451ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:32:28.926892Z","caller":"traceutil/trace.go:172","msg":"trace[2005831791] transaction","detail":"{read_only:false; response_revision:921; number_of_response:1; }","duration":"233.818842ms","start":"2025-10-20T04:32:28.694704Z","end":"2025-10-20T04:32:28.926880Z","steps":["trace[2005831791] 'process raft request'  (duration: 35.124411ms)","trace[2005831791] 'compare'  (duration: 197.566844ms)"],"step_count":2}
{"level":"info","ts":"2025-10-20T04:32:31.506669Z","caller":"traceutil/trace.go:172","msg":"trace[1719580377] transaction","detail":"{read_only:false; response_revision:923; number_of_response:1; }","duration":"115.520314ms","start":"2025-10-20T04:32:31.391137Z","end":"2025-10-20T04:32:31.506657Z","steps":["trace[1719580377] 'process raft request'  (duration: 115.366302ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:32:32.502319Z","caller":"traceutil/trace.go:172","msg":"trace[1328292615] transaction","detail":"{read_only:false; response_revision:924; number_of_response:1; }","duration":"126.892192ms","start":"2025-10-20T04:32:32.375414Z","end":"2025-10-20T04:32:32.502306Z","steps":["trace[1328292615] 'process raft request'  (duration: 126.751881ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:32:35.695862Z","caller":"traceutil/trace.go:172","msg":"trace[1641207580] transaction","detail":"{read_only:false; response_revision:927; number_of_response:1; }","duration":"126.618215ms","start":"2025-10-20T04:32:35.569231Z","end":"2025-10-20T04:32:35.695849Z","steps":["trace[1641207580] 'process raft request'  (duration: 126.355295ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:32:41.938837Z","caller":"traceutil/trace.go:172","msg":"trace[337673598] transaction","detail":"{read_only:false; response_revision:931; number_of_response:1; }","duration":"106.343944ms","start":"2025-10-20T04:32:41.832478Z","end":"2025-10-20T04:32:41.938822Z","steps":["trace[337673598] 'process raft request'  (duration: 106.121826ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:32:42.440814Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"320.104715ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:32:42.440939Z","caller":"traceutil/trace.go:172","msg":"trace[690866711] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:931; }","duration":"320.245025ms","start":"2025-10-20T04:32:42.120682Z","end":"2025-10-20T04:32:42.440927Z","steps":["trace[690866711] 'range keys from in-memory index tree'  (duration: 320.067112ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:32:42.688497Z","caller":"traceutil/trace.go:172","msg":"trace[761469795] transaction","detail":"{read_only:false; response_revision:932; number_of_response:1; }","duration":"122.880026ms","start":"2025-10-20T04:32:42.565603Z","end":"2025-10-20T04:32:42.688483Z","steps":["trace[761469795] 'process raft request'  (duration: 122.692612ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:32:43.056874Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"151.800973ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:32:43.056949Z","caller":"traceutil/trace.go:172","msg":"trace[1678986327] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:932; }","duration":"151.886079ms","start":"2025-10-20T04:32:42.905052Z","end":"2025-10-20T04:32:43.056938Z","steps":["trace[1678986327] 'range keys from in-memory index tree'  (duration: 137.635275ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:32:43.057194Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"137.897294ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128040751784692664 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:924 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:16"}
{"level":"info","ts":"2025-10-20T04:32:43.057284Z","caller":"traceutil/trace.go:172","msg":"trace[1652747994] transaction","detail":"{read_only:false; response_revision:933; number_of_response:1; }","duration":"435.091433ms","start":"2025-10-20T04:32:42.622184Z","end":"2025-10-20T04:32:43.057275Z","steps":["trace[1652747994] 'process raft request'  (duration: 297.044827ms)","trace[1652747994] 'compare'  (duration: 137.572269ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-20T04:32:43.057320Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-20T04:32:42.622161Z","time spent":"435.140637ms","remote":"127.0.0.1:60358","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:924 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2025-10-20T04:32:43.917099Z","caller":"traceutil/trace.go:172","msg":"trace[38758604] linearizableReadLoop","detail":"{readStateIndex:1051; appliedIndex:1051; }","duration":"136.625696ms","start":"2025-10-20T04:32:43.780455Z","end":"2025-10-20T04:32:43.917080Z","steps":["trace[38758604] 'read index received'  (duration: 136.619196ms)","trace[38758604] 'applied index is now lower than readState.Index'  (duration: 5µs)"],"step_count":2}
{"level":"warn","ts":"2025-10-20T04:32:44.382029Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"464.906857ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128040751784692669 > lease_revoke:<id:70cc99ffe2068b74>","response":"size:29"}
{"level":"warn","ts":"2025-10-20T04:32:44.382078Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"261.764602ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:32:44.382111Z","caller":"traceutil/trace.go:172","msg":"trace[1141494902] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:933; }","duration":"261.808505ms","start":"2025-10-20T04:32:44.120293Z","end":"2025-10-20T04:32:44.382102Z","steps":["trace[1141494902] 'range keys from in-memory index tree'  (duration: 261.725099ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:32:44.382114Z","caller":"traceutil/trace.go:172","msg":"trace[390258295] linearizableReadLoop","detail":"{readStateIndex:1052; appliedIndex:1051; }","duration":"438.575015ms","start":"2025-10-20T04:32:43.943532Z","end":"2025-10-20T04:32:44.382107Z","steps":["trace[390258295] 'read index received'  (duration: 28.802µs)","trace[390258295] 'applied index is now lower than readState.Index'  (duration: 438.545613ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-20T04:32:44.382183Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"601.727268ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/mutatingwebhookconfigurations\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:32:44.382197Z","caller":"traceutil/trace.go:172","msg":"trace[1298561992] range","detail":"{range_begin:/registry/mutatingwebhookconfigurations; range_end:; response_count:0; response_revision:933; }","duration":"601.741769ms","start":"2025-10-20T04:32:43.780451Z","end":"2025-10-20T04:32:44.382192Z","steps":["trace[1298561992] 'agreement among raft nodes before linearized reading'  (duration: 136.726304ms)","trace[1298561992] 'range keys from in-memory index tree'  (duration: 464.989263ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-20T04:32:44.382214Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"438.703224ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"warn","ts":"2025-10-20T04:32:44.382219Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-20T04:32:43.780431Z","time spent":"601.781273ms","remote":"127.0.0.1:60838","response type":"/etcdserverpb.KV/Range","request count":0,"request size":43,"response count":0,"response size":29,"request content":"key:\"/registry/mutatingwebhookconfigurations\" limit:1 "}
{"level":"info","ts":"2025-10-20T04:32:44.382231Z","caller":"traceutil/trace.go:172","msg":"trace[1245501281] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:933; }","duration":"438.721926ms","start":"2025-10-20T04:32:43.943504Z","end":"2025-10-20T04:32:44.382225Z","steps":["trace[1245501281] 'agreement among raft nodes before linearized reading'  (duration: 438.628419ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:32:44.382247Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-20T04:32:43.943487Z","time spent":"438.755029ms","remote":"127.0.0.1:46890","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"info","ts":"2025-10-20T04:32:44.949925Z","caller":"traceutil/trace.go:172","msg":"trace[151174731] linearizableReadLoop","detail":"{readStateIndex:1052; appliedIndex:1052; }","duration":"482.782244ms","start":"2025-10-20T04:32:44.467123Z","end":"2025-10-20T04:32:44.949905Z","steps":["trace[151174731] 'read index received'  (duration: 482.773243ms)","trace[151174731] 'applied index is now lower than readState.Index'  (duration: 8.101µs)"],"step_count":2}
{"level":"warn","ts":"2025-10-20T04:32:44.950082Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"482.959957ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/runtimeclasses\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-10-20T04:32:44.950108Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"125.890364ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:32:44.950207Z","caller":"traceutil/trace.go:172","msg":"trace[125750939] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:934; }","duration":"125.991271ms","start":"2025-10-20T04:32:44.824207Z","end":"2025-10-20T04:32:44.950198Z","steps":["trace[125750939] 'agreement among raft nodes before linearized reading'  (duration: 125.873562ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:32:44.950122Z","caller":"traceutil/trace.go:172","msg":"trace[1994760486] range","detail":"{range_begin:/registry/runtimeclasses; range_end:; response_count:0; response_revision:933; }","duration":"483.017362ms","start":"2025-10-20T04:32:44.467096Z","end":"2025-10-20T04:32:44.950113Z","steps":["trace[1994760486] 'agreement among raft nodes before linearized reading'  (duration: 482.928555ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:32:44.950309Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-20T04:32:44.467078Z","time spent":"483.217877ms","remote":"127.0.0.1:60492","response type":"/etcdserverpb.KV/Range","request count":0,"request size":28,"response count":0,"response size":29,"request content":"key:\"/registry/runtimeclasses\" limit:1 "}
{"level":"info","ts":"2025-10-20T04:32:44.950149Z","caller":"traceutil/trace.go:172","msg":"trace[1203624704] transaction","detail":"{read_only:false; response_revision:934; number_of_response:1; }","duration":"564.652793ms","start":"2025-10-20T04:32:44.385490Z","end":"2025-10-20T04:32:44.950143Z","steps":["trace[1203624704] 'process raft request'  (duration: 564.408174ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:32:44.950556Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-20T04:32:44.385472Z","time spent":"565.013121ms","remote":"127.0.0.1:46890","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:931 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-10-20T04:32:45.753312Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"633.020795ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-10-20T04:32:45.753345Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"502.235652ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingwebhookconfigurations\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:32:45.753372Z","caller":"traceutil/trace.go:172","msg":"trace[1023329956] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:934; }","duration":"633.091101ms","start":"2025-10-20T04:32:45.120270Z","end":"2025-10-20T04:32:45.753361Z","steps":["trace[1023329956] 'range keys from in-memory index tree'  (duration: 632.984392ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:32:45.753381Z","caller":"traceutil/trace.go:172","msg":"trace[8147966] range","detail":"{range_begin:/registry/validatingwebhookconfigurations; range_end:; response_count:0; response_revision:934; }","duration":"502.276556ms","start":"2025-10-20T04:32:45.251097Z","end":"2025-10-20T04:32:45.753374Z","steps":["trace[8147966] 'range keys from in-memory index tree'  (duration: 502.181048ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:32:45.753409Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-20T04:32:45.251074Z","time spent":"502.328659ms","remote":"127.0.0.1:60834","response type":"/etcdserverpb.KV/Range","request count":0,"request size":45,"response count":0,"response size":29,"request content":"key:\"/registry/validatingwebhookconfigurations\" limit:1 "}
{"level":"info","ts":"2025-10-20T04:32:47.177664Z","caller":"traceutil/trace.go:172","msg":"trace[1603735842] transaction","detail":"{read_only:false; response_revision:935; number_of_response:1; }","duration":"218.500247ms","start":"2025-10-20T04:32:46.959149Z","end":"2025-10-20T04:32:47.177649Z","steps":["trace[1603735842] 'process raft request'  (duration: 218.225725ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:32:49.076566Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"252.339771ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-10-20T04:32:49.076643Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"233.482608ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128040751784692699 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:929 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128040751784692697 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-10-20T04:32:49.076648Z","caller":"traceutil/trace.go:172","msg":"trace[209779733] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:935; }","duration":"252.429478ms","start":"2025-10-20T04:32:48.824203Z","end":"2025-10-20T04:32:49.076633Z","steps":["trace[209779733] 'agreement among raft nodes before linearized reading'  (duration: 18.849362ms)","trace[209779733] 'range keys from in-memory index tree'  (duration: 233.462707ms)"],"step_count":2}
{"level":"info","ts":"2025-10-20T04:32:49.076704Z","caller":"traceutil/trace.go:172","msg":"trace[1722943558] transaction","detail":"{read_only:false; response_revision:936; number_of_response:1; }","duration":"304.227896ms","start":"2025-10-20T04:32:48.772464Z","end":"2025-10-20T04:32:49.076692Z","steps":["trace[1722943558] 'process raft request'  (duration: 70.629978ms)","trace[1722943558] 'compare'  (duration: 233.392002ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-20T04:32:49.076752Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-20T04:32:48.772441Z","time spent":"304.284099ms","remote":"127.0.0.1:46578","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:929 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128040751784692697 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2025-10-20T04:32:49.447952Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"262.817183ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-10-20T04:32:49.448049Z","caller":"traceutil/trace.go:172","msg":"trace[999490470] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:936; }","duration":"262.955894ms","start":"2025-10-20T04:32:49.185081Z","end":"2025-10-20T04:32:49.448037Z","steps":["trace[999490470] 'range keys from in-memory index tree'  (duration: 262.654571ms)"],"step_count":1}
{"level":"info","ts":"2025-10-20T04:32:49.664887Z","caller":"traceutil/trace.go:172","msg":"trace[431868139] transaction","detail":"{read_only:false; response_revision:937; number_of_response:1; }","duration":"211.828429ms","start":"2025-10-20T04:32:49.453044Z","end":"2025-10-20T04:32:49.664872Z","steps":["trace[431868139] 'process raft request'  (duration: 211.493503ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:32:50.401854Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"281.075099ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:32:50.401929Z","caller":"traceutil/trace.go:172","msg":"trace[1690086329] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:937; }","duration":"281.159206ms","start":"2025-10-20T04:32:50.120758Z","end":"2025-10-20T04:32:50.401917Z","steps":["trace[1690086329] 'range keys from in-memory index tree'  (duration: 281.009994ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:32:50.401892Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"577.284773ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-20T04:32:50.402035Z","caller":"traceutil/trace.go:172","msg":"trace[1437898901] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:937; }","duration":"577.426584ms","start":"2025-10-20T04:32:49.824597Z","end":"2025-10-20T04:32:50.402023Z","steps":["trace[1437898901] 'range keys from in-memory index tree'  (duration: 577.194965ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-20T04:32:50.402107Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-20T04:32:49.824574Z","time spent":"577.516891ms","remote":"127.0.0.1:46536","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}


==> kernel <==
 04:32:51 up  1:02,  0 users,  load average: 1.03, 1.59, 1.03
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [d6ea0b14b7a6] <==
I1020 04:30:37.732827       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1020 04:30:37.732834       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1020 04:30:37.732932       1 controller.go:78] Starting OpenAPI AggregationController
I1020 04:30:37.733517       1 controller.go:119] Starting legacy_token_tracking_controller
I1020 04:30:37.733531       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
I1020 04:30:37.734057       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I1020 04:30:37.734093       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I1020 04:30:37.734137       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1020 04:30:37.734238       1 remote_available_controller.go:425] Starting RemoteAvailability controller
I1020 04:30:37.734270       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1020 04:30:37.734866       1 local_available_controller.go:156] Starting LocalAvailability controller
I1020 04:30:37.734879       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1020 04:30:37.734896       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1020 04:30:37.734966       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1020 04:30:37.737701       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1020 04:30:37.737787       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I1020 04:30:37.737816       1 aggregator.go:169] waiting for initial CRD sync...
I1020 04:30:37.737852       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1020 04:30:37.732443       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1020 04:30:37.749672       1 controller.go:142] Starting OpenAPI controller
I1020 04:30:37.749715       1 controller.go:90] Starting OpenAPI V3 controller
I1020 04:30:37.749731       1 naming_controller.go:299] Starting NamingConditionController
I1020 04:30:37.749743       1 establishing_controller.go:81] Starting EstablishingController
I1020 04:30:37.749755       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1020 04:30:37.749767       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1020 04:30:37.749784       1 crd_finalizer.go:269] Starting CRDFinalizer
I1020 04:30:37.749809       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1020 04:30:37.749819       1 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
I1020 04:30:37.812859       1 repairip.go:210] Starting ipallocator-repair-controller
I1020 04:30:37.814409       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I1020 04:30:37.834517       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1020 04:30:37.845526       1 default_servicecidr_controller.go:111] Starting kubernetes-service-cidr-controller
I1020 04:30:37.845844       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I1020 04:30:37.894867       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1020 04:30:37.895054       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1020 04:30:37.895165       1 policy_source.go:240] refreshing policies
I1020 04:30:37.895537       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1020 04:30:37.895980       1 aggregator.go:171] initial CRD sync complete...
I1020 04:30:37.896000       1 autoregister_controller.go:144] Starting autoregister controller
I1020 04:30:37.896006       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1020 04:30:37.896012       1 cache.go:39] Caches are synced for autoregister controller
I1020 04:30:37.911455       1 cache.go:39] Caches are synced for LocalAvailability controller
I1020 04:30:37.915218       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1020 04:30:37.915319       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1020 04:30:37.927094       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1020 04:30:37.933840       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1020 04:30:37.994437       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1020 04:30:37.938652       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1020 04:30:37.938631       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1020 04:30:37.938613       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1020 04:30:37.994678       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1020 04:30:38.002309       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1020 04:30:38.002568       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1020 04:30:39.368414       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1020 04:30:40.934322       1 controller.go:667] quota admission added evaluator for: endpoints
I1020 04:31:12.690274       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1020 04:31:14.268468       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1020 04:31:41.696290       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1020 04:31:45.731399       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1020 04:32:43.410579       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-apiserver [fdb54ad7aea7] <==
{"level":"warn","ts":"2025-10-20T04:30:02.568881Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000337a40/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = latest balancer error: connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\""}
W1020 04:30:02.747791       1 logging.go:55] [core] [Channel #187 SubChannel #189]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:02.764893       1 logging.go:55] [core] [Channel #139 SubChannel #141]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:02.929660       1 logging.go:55] [core] [Channel #171 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.105759       1 logging.go:55] [core] [Channel #255 SubChannel #257]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.143761       1 logging.go:55] [core] [Channel #39 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.190564       1 logging.go:55] [core] [Channel #99 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.224830       1 logging.go:55] [core] [Channel #151 SubChannel #153]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.285975       1 logging.go:55] [core] [Channel #43 SubChannel #45]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.308423       1 logging.go:55] [core] [Channel #219 SubChannel #221]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.328590       1 logging.go:55] [core] [Channel #247 SubChannel #249]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.372836       1 logging.go:55] [core] [Channel #235 SubChannel #237]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.419623       1 logging.go:55] [core] [Channel #95 SubChannel #97]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.469242       1 logging.go:55] [core] [Channel #227 SubChannel #229]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.567629       1 logging.go:55] [core] [Channel #183 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.646345       1 logging.go:55] [core] [Channel #131 SubChannel #133]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.649953       1 logging.go:55] [core] [Channel #123 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.688987       1 logging.go:55] [core] [Channel #4 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.697904       1 logging.go:55] [core] [Channel #211 SubChannel #213]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.736956       1 logging.go:55] [core] [Channel #47 SubChannel #49]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.761653       1 logging.go:55] [core] [Channel #13 SubChannel #15]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.771665       1 logging.go:55] [core] [Channel #111 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.804874       1 logging.go:55] [core] [Channel #251 SubChannel #253]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.805153       1 logging.go:55] [core] [Channel #71 SubChannel #73]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.834634       1 logging.go:55] [core] [Channel #135 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.847582       1 logging.go:55] [core] [Channel #59 SubChannel #61]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.871508       1 logging.go:55] [core] [Channel #147 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.895875       1 logging.go:55] [core] [Channel #91 SubChannel #93]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.907447       1 logging.go:55] [core] [Channel #115 SubChannel #117]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.930491       1 logging.go:55] [core] [Channel #159 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:03.933123       1 logging.go:55] [core] [Channel #143 SubChannel #145]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.031613       1 logging.go:55] [core] [Channel #231 SubChannel #233]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.046669       1 logging.go:55] [core] [Channel #103 SubChannel #105]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.133152       1 logging.go:55] [core] [Channel #243 SubChannel #245]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.135682       1 logging.go:55] [core] [Channel #27 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.161992       1 logging.go:55] [core] [Channel #167 SubChannel #169]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.181661       1 logging.go:55] [core] [Channel #179 SubChannel #181]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.224390       1 logging.go:55] [core] [Channel #22 SubChannel #24]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.234152       1 logging.go:55] [core] [Channel #207 SubChannel #209]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.241807       1 logging.go:55] [core] [Channel #175 SubChannel #177]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.250978       1 logging.go:55] [core] [Channel #163 SubChannel #165]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.302196       1 logging.go:55] [core] [Channel #239 SubChannel #241]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.306053       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.358204       1 logging.go:55] [core] [Channel #155 SubChannel #157]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.362832       1 logging.go:55] [core] [Channel #107 SubChannel #109]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.377778       1 logging.go:55] [core] [Channel #75 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.382376       1 logging.go:55] [core] [Channel #79 SubChannel #81]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.463085       1 logging.go:55] [core] [Channel #119 SubChannel #121]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.470778       1 logging.go:55] [core] [Channel #203 SubChannel #205]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.472245       1 logging.go:55] [core] [Channel #195 SubChannel #197]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.500633       1 logging.go:55] [core] [Channel #31 SubChannel #33]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.520150       1 logging.go:55] [core] [Channel #127 SubChannel #129]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.525929       1 logging.go:55] [core] [Channel #55 SubChannel #57]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
{"level":"warn","ts":"2025-10-20T04:30:04.569989Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0005ecf00/127.0.0.1:2379","method":"/etcdserverpb.KV/Range","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = latest balancer error: connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\""}
W1020 04:30:04.576389       1 logging.go:55] [core] [Channel #67 SubChannel #69]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.585498       1 logging.go:55] [core] [Channel #35 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.629357       1 logging.go:55] [core] [Channel #215 SubChannel #217]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.655436       1 logging.go:55] [core] [Channel #63 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.666358       1 logging.go:55] [core] [Channel #191 SubChannel #193]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1020 04:30:04.802863       1 logging.go:55] [core] [Channel #223 SubChannel #225]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [0e4ebd7545e1] <==
I1020 04:31:13.998392       1 vac_protection_controller.go:206] "Starting VAC protection controller" logger="volumeattributesclass-protection-controller"
I1020 04:31:13.998415       1 shared_informer.go:349] "Waiting for caches to sync" controller="VAC protection"
I1020 04:31:14.001541       1 controllermanager.go:781] "Started controller" controller="replicationcontroller-controller"
I1020 04:31:14.001837       1 replica_set.go:243] "Starting controller" logger="replicationcontroller-controller" name="replicationcontroller"
I1020 04:31:14.001907       1 shared_informer.go:349] "Waiting for caches to sync" controller="ReplicationController"
I1020 04:31:14.017813       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1020 04:31:14.096014       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1020 04:31:14.114720       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1020 04:31:14.120232       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1020 04:31:14.241810       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1020 04:31:14.253473       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1020 04:31:14.257024       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1020 04:31:14.257201       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1020 04:31:14.257733       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1020 04:31:14.260516       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1020 04:31:14.274502       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1020 04:31:14.278063       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1020 04:31:14.281497       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1020 04:31:14.288877       1 shared_informer.go:356] "Caches are synced" controller="node"
I1020 04:31:14.288982       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1020 04:31:14.289053       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1020 04:31:14.289080       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1020 04:31:14.289088       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1020 04:31:14.292445       1 shared_informer.go:356] "Caches are synced" controller="job"
I1020 04:31:14.295117       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1020 04:31:14.295612       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1020 04:31:14.296267       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1020 04:31:14.298682       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1020 04:31:14.298804       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1020 04:31:14.299616       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1020 04:31:14.304881       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1020 04:31:14.304938       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1020 04:31:14.304954       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1020 04:31:14.306490       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1020 04:31:14.308828       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1020 04:31:14.309087       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1020 04:31:14.309105       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1020 04:31:14.311297       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1020 04:31:14.311313       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1020 04:31:14.311324       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1020 04:31:14.312044       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1020 04:31:14.313966       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1020 04:31:14.311332       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1020 04:31:14.314067       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1020 04:31:14.314075       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1020 04:31:14.314081       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1020 04:31:14.313911       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1020 04:31:14.313916       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1020 04:31:14.317758       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1020 04:31:14.318313       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1020 04:31:14.322742       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1020 04:31:14.323781       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1020 04:31:14.323943       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1020 04:31:14.323986       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1020 04:31:14.327333       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1020 04:31:14.330272       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1020 04:31:14.334377       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1020 04:31:14.338676       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1020 04:31:14.341126       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1020 04:31:14.342369       1 shared_informer.go:356] "Caches are synced" controller="attach detach"


==> kube-controller-manager [79e298e73d41] <==
I1020 04:30:33.426014       1 serving.go:386] Generated self-signed cert in-memory
I1020 04:30:37.541388       1 controllermanager.go:191] "Starting" version="v1.34.0"
I1020 04:30:37.541484       1 controllermanager.go:193] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1020 04:30:37.596661       1 secure_serving.go:211] Serving securely on 127.0.0.1:10257
I1020 04:30:37.597224       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1020 04:30:37.597326       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1020 04:30:37.597390       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
E1020 04:30:47.932091       1 controllermanager.go:245] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: an error on the server (\"[+]ping ok\\n[+]log ok\\n[+]etcd ok\\n[+]poststarthook/start-apiserver-admission-initializer ok\\n[+]poststarthook/generic-apiserver-start-informers ok\\n[+]poststarthook/priority-and-fairness-config-consumer ok\\n[+]poststarthook/priority-and-fairness-filter ok\\n[+]poststarthook/storage-object-count-tracker-hook ok\\n[+]poststarthook/start-apiextensions-informers ok\\n[+]poststarthook/start-apiextensions-controllers ok\\n[+]poststarthook/crd-informer-synced ok\\n[+]poststarthook/start-system-namespaces-controller ok\\n[+]poststarthook/start-cluster-authentication-info-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok\\n[+]poststarthook/start-legacy-token-tracking-controller ok\\n[+]poststarthook/start-service-ip-repair-controllers ok\\n[-]poststarthook/rbac/bootstrap-roles failed: reason withheld\\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\\n[+]poststarthook/priority-and-fairness-config-producer ok\\n[+]poststarthook/bootstrap-controller ok\\n[+]poststarthook/start-kubernetes-service-cidr-controller ok\\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\\n[+]poststarthook/start-kube-aggregator-informers ok\\n[+]poststarthook/apiservice-status-local-available-controller ok\\n[+]poststarthook/apiservice-status-remote-available-controller ok\\n[+]poststarthook/apiservice-registration-controller ok\\n[+]poststarthook/apiservice-discovery-controller ok\\n[+]poststarthook/kube-apiserver-autoregistration ok\\n[+]autoregister-completion ok\\n[+]poststarthook/apiservice-openapi-controller ok\\n[+]poststarthook/apiservice-openapiv3-controller ok\\nhealthz check failed\") has prevented the request from succeeding"


==> kube-proxy [b12876f6c9ec] <==
I1020 04:24:28.495920       1 server_linux.go:53] "Using iptables proxy"
I1020 04:24:28.661192       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1020 04:24:28.762340       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1020 04:24:28.762385       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1020 04:24:28.762488       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1020 04:24:28.807399       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1020 04:24:28.807874       1 server_linux.go:132] "Using iptables Proxier"
I1020 04:24:28.822994       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1020 04:24:28.975528       1 server.go:527] "Version info" version="v1.34.0"
I1020 04:24:28.975586       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1020 04:24:28.979651       1 config.go:200] "Starting service config controller"
I1020 04:24:28.980494       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1020 04:24:28.980056       1 config.go:309] "Starting node config controller"
I1020 04:24:28.980563       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1020 04:24:28.980570       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1020 04:24:28.980103       1 config.go:403] "Starting serviceCIDR config controller"
I1020 04:24:28.980580       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1020 04:24:28.980070       1 config.go:106] "Starting endpoint slice config controller"
I1020 04:24:28.983180       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1020 04:24:29.081682       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1020 04:24:29.083489       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1020 04:24:29.083629       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"


==> kube-proxy [fbebf127e9aa] <==
I1020 04:30:31.926985       1 server_linux.go:53] "Using iptables proxy"
I1020 04:30:32.132360       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
E1020 04:30:32.133641       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
I1020 04:30:38.055986       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1020 04:30:38.056040       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1020 04:30:38.056123       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1020 04:30:38.594823       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1020 04:30:38.595120       1 server_linux.go:132] "Using iptables Proxier"
I1020 04:30:38.621901       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1020 04:30:38.622950       1 server.go:527] "Version info" version="v1.34.0"
I1020 04:30:38.624389       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1020 04:30:38.714741       1 config.go:200] "Starting service config controller"
I1020 04:30:38.714799       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1020 04:30:38.715072       1 config.go:106] "Starting endpoint slice config controller"
I1020 04:30:38.715090       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1020 04:30:38.715117       1 config.go:403] "Starting serviceCIDR config controller"
I1020 04:30:38.715122       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1020 04:30:38.728696       1 config.go:309] "Starting node config controller"
I1020 04:30:38.737014       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1020 04:30:38.737284       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1020 04:30:38.815906       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1020 04:30:38.815960       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1020 04:30:38.816005       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [2696b2dbacf7] <==
E1020 04:23:24.573497       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1020 04:23:24.588188       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1020 04:23:25.818749       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1020 04:23:25.954202       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1020 04:23:26.092125       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1020 04:23:26.181667       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1020 04:23:26.190153       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1020 04:23:26.195626       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1020 04:23:26.221283       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1020 04:23:26.278930       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1020 04:23:26.583950       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1020 04:23:26.651782       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1020 04:23:26.816363       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1020 04:23:26.817564       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1020 04:23:26.932556       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1020 04:23:26.969689       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1020 04:23:26.974789       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1020 04:23:27.098790       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1020 04:23:27.111776       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1020 04:23:27.278887       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1020 04:23:27.375318       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1020 04:23:29.106946       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1020 04:23:29.888225       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1020 04:23:30.362543       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1020 04:23:30.368365       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1020 04:23:31.098100       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1020 04:23:31.248439       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1020 04:23:31.513197       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1020 04:23:31.672957       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1020 04:23:31.737059       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1020 04:23:31.780567       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1020 04:23:31.995328       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1020 04:23:32.298077       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1020 04:23:32.379471       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1020 04:23:32.491124       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1020 04:23:32.573867       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1020 04:23:32.594672       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1020 04:23:32.607269       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1020 04:23:32.693305       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1020 04:23:32.786944       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1020 04:23:35.645484       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1020 04:23:36.451159       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1020 04:23:39.402527       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1020 04:23:39.445108       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1020 04:23:40.162079       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1020 04:23:40.235386       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1020 04:23:40.883565       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1020 04:23:40.946158       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1020 04:23:40.956805       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1020 04:23:41.081814       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1020 04:23:41.466533       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1020 04:23:41.480115       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1020 04:23:41.783342       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1020 04:23:43.521417       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I1020 04:23:59.369330       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1020 04:29:54.520461       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I1020 04:29:54.521074       1 secure_serving.go:259] Stopped listening on 127.0.0.1:10259
I1020 04:29:54.521089       1 server.go:263] "[graceful-termination] secure server has stopped listening"
I1020 04:29:54.521138       1 server.go:265] "[graceful-termination] secure server is exiting"
E1020 04:29:54.521154       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [57a0acec854a] <==
I1020 04:30:33.597063       1 serving.go:386] Generated self-signed cert in-memory
W1020 04:30:37.842383       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1020 04:30:37.842808       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found]
W1020 04:30:37.842830       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1020 04:30:37.842838       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1020 04:30:38.216815       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1020 04:30:38.217145       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1020 04:30:38.228048       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1020 04:30:38.228502       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1020 04:30:38.229850       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1020 04:30:38.229996       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1020 04:30:38.409094       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Oct 20 04:30:13 minikube kubelet[2209]: E1020 04:30:13.347292    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="3b51c8241e224d47681cce32ea99b407" pod="kube-system/kube-controller-manager-minikube"
Oct 20 04:30:14 minikube kubelet[2209]: E1020 04:30:14.422191    2209 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = failed to create a sandbox for pod \"kube-apiserver-minikube\": Error response from daemon: Conflict. The container name \"/k8s_POD_kube-apiserver-minikube_kube-system_8312b4cdc4b705c0e12f63794469cfad_1\" is already in use by container \"c135e29c2233c97a837a56cbe353c252b37ba5df6ead734a9ba71cd8b47dd729\". You have to remove (or rename) that container to be able to reuse that name."
Oct 20 04:30:14 minikube kubelet[2209]: E1020 04:30:14.422253    2209 kuberuntime_sandbox.go:71] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed to create a sandbox for pod \"kube-apiserver-minikube\": Error response from daemon: Conflict. The container name \"/k8s_POD_kube-apiserver-minikube_kube-system_8312b4cdc4b705c0e12f63794469cfad_1\" is already in use by container \"c135e29c2233c97a837a56cbe353c252b37ba5df6ead734a9ba71cd8b47dd729\". You have to remove (or rename) that container to be able to reuse that name." pod="kube-system/kube-apiserver-minikube"
Oct 20 04:30:14 minikube kubelet[2209]: E1020 04:30:14.422278    2209 kuberuntime_manager.go:1343] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed to create a sandbox for pod \"kube-apiserver-minikube\": Error response from daemon: Conflict. The container name \"/k8s_POD_kube-apiserver-minikube_kube-system_8312b4cdc4b705c0e12f63794469cfad_1\" is already in use by container \"c135e29c2233c97a837a56cbe353c252b37ba5df6ead734a9ba71cd8b47dd729\". You have to remove (or rename) that container to be able to reuse that name." pod="kube-system/kube-apiserver-minikube"
Oct 20 04:30:14 minikube kubelet[2209]: E1020 04:30:14.422336    2209 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-apiserver-minikube_kube-system(8312b4cdc4b705c0e12f63794469cfad)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"kube-apiserver-minikube_kube-system(8312b4cdc4b705c0e12f63794469cfad)\\\": rpc error: code = Unknown desc = failed to create a sandbox for pod \\\"kube-apiserver-minikube\\\": Error response from daemon: Conflict. The container name \\\"/k8s_POD_kube-apiserver-minikube_kube-system_8312b4cdc4b705c0e12f63794469cfad_1\\\" is already in use by container \\\"c135e29c2233c97a837a56cbe353c252b37ba5df6ead734a9ba71cd8b47dd729\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/kube-apiserver-minikube" podUID="8312b4cdc4b705c0e12f63794469cfad"
Oct 20 04:30:14 minikube kubelet[2209]: I1020 04:30:14.429348    2209 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c135e29c2233c97a837a56cbe353c252b37ba5df6ead734a9ba71cd8b47dd729"
Oct 20 04:30:16 minikube kubelet[2209]: I1020 04:30:16.481080    2209 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="851ab088479257a2c1ad9b317ff120c74d0120d82127359b0cd2edb4173587a7"
Oct 20 04:30:16 minikube kubelet[2209]: E1020 04:30:16.750674    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="0c83f630-643b-414c-b6bb-294a9e466f53" pod="kube-system/storage-provisioner"
Oct 20 04:30:16 minikube kubelet[2209]: E1020 04:30:16.751898    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-proxy-2snft\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="0d495b3a-031f-44cf-8e2a-0d49e8c2dd03" pod="kube-system/kube-proxy-2snft"
Oct 20 04:30:16 minikube kubelet[2209]: E1020 04:30:16.752301    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/coredns-66bc5c9577-nhrn7\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="eea835d9-a82e-45f1-92ee-d99de871d32a" pod="kube-system/coredns-66bc5c9577-nhrn7"
Oct 20 04:30:16 minikube kubelet[2209]: E1020 04:30:16.752658    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="dc6cf0a7bcb54d1f95cecc4d7b6b7d67" pod="kube-system/kube-scheduler-minikube"
Oct 20 04:30:16 minikube kubelet[2209]: E1020 04:30:16.753033    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="3b51c8241e224d47681cce32ea99b407" pod="kube-system/kube-controller-manager-minikube"
Oct 20 04:30:16 minikube kubelet[2209]: E1020 04:30:16.753341    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="8312b4cdc4b705c0e12f63794469cfad" pod="kube-system/kube-apiserver-minikube"
Oct 20 04:30:16 minikube kubelet[2209]: E1020 04:30:16.753566    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="e3a36fac0ae701bc11fad0a6716eec2c" pod="kube-system/etcd-minikube"
Oct 20 04:30:17 minikube kubelet[2209]: E1020 04:30:17.502037    2209 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://192.168.49.2:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="7s"
Oct 20 04:30:19 minikube kubelet[2209]: E1020 04:30:19.704706    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/coredns-66bc5c9577-nhrn7\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="eea835d9-a82e-45f1-92ee-d99de871d32a" pod="kube-system/coredns-66bc5c9577-nhrn7"
Oct 20 04:30:19 minikube kubelet[2209]: E1020 04:30:19.704997    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="dc6cf0a7bcb54d1f95cecc4d7b6b7d67" pod="kube-system/kube-scheduler-minikube"
Oct 20 04:30:19 minikube kubelet[2209]: E1020 04:30:19.705193    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="3b51c8241e224d47681cce32ea99b407" pod="kube-system/kube-controller-manager-minikube"
Oct 20 04:30:19 minikube kubelet[2209]: E1020 04:30:19.705344    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="8312b4cdc4b705c0e12f63794469cfad" pod="kube-system/kube-apiserver-minikube"
Oct 20 04:30:19 minikube kubelet[2209]: E1020 04:30:19.705538    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="e3a36fac0ae701bc11fad0a6716eec2c" pod="kube-system/etcd-minikube"
Oct 20 04:30:19 minikube kubelet[2209]: E1020 04:30:19.705772    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="0c83f630-643b-414c-b6bb-294a9e466f53" pod="kube-system/storage-provisioner"
Oct 20 04:30:19 minikube kubelet[2209]: E1020 04:30:19.705949    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-proxy-2snft\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="0d495b3a-031f-44cf-8e2a-0d49e8c2dd03" pod="kube-system/kube-proxy-2snft"
Oct 20 04:30:21 minikube kubelet[2209]: E1020 04:30:21.335790    2209 kubelet_node_status.go:486] "Error updating node status, will retry" err="failed to patch status \"{\\\"status\\\":{\\\"$setElementOrder/conditions\\\":[{\\\"type\\\":\\\"MemoryPressure\\\"},{\\\"type\\\":\\\"DiskPressure\\\"},{\\\"type\\\":\\\"PIDPressure\\\"},{\\\"type\\\":\\\"Ready\\\"}],\\\"conditions\\\":[{\\\"lastHeartbeatTime\\\":\\\"2025-10-20T04:30:21Z\\\",\\\"type\\\":\\\"MemoryPressure\\\"},{\\\"lastHeartbeatTime\\\":\\\"2025-10-20T04:30:21Z\\\",\\\"type\\\":\\\"DiskPressure\\\"},{\\\"lastHeartbeatTime\\\":\\\"2025-10-20T04:30:21Z\\\",\\\"type\\\":\\\"PIDPressure\\\"},{\\\"lastHeartbeatTime\\\":\\\"2025-10-20T04:30:21Z\\\",\\\"type\\\":\\\"Ready\\\"}],\\\"nodeInfo\\\":{\\\"containerRuntimeVersion\\\":\\\"docker://28.4.0\\\"}}}\" for node \"minikube\": Patch \"https://192.168.49.2:8443/api/v1/nodes/minikube/status?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 20 04:30:21 minikube kubelet[2209]: E1020 04:30:21.336084    2209 kubelet_node_status.go:486] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://192.168.49.2:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 20 04:30:21 minikube kubelet[2209]: E1020 04:30:21.336251    2209 kubelet_node_status.go:486] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://192.168.49.2:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 20 04:30:21 minikube kubelet[2209]: E1020 04:30:21.336396    2209 kubelet_node_status.go:486] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://192.168.49.2:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 20 04:30:21 minikube kubelet[2209]: E1020 04:30:21.336552    2209 kubelet_node_status.go:486] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://192.168.49.2:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 20 04:30:21 minikube kubelet[2209]: E1020 04:30:21.336585    2209 kubelet_node_status.go:473] "Unable to update node status" err="update node status exceeds retry count"
Oct 20 04:30:21 minikube kubelet[2209]: I1020 04:30:21.615821    2209 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a7b6f25138d571e116dfd23072c4d126e871174910445e743d19ef7b1badbb7b"
Oct 20 04:30:22 minikube kubelet[2209]: E1020 04:30:22.753150    2209 event.go:368] "Unable to write event (may retry after sleeping)" err="Patch \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/events/kube-apiserver-minikube.1870185f2c73c7a8\": dial tcp 192.168.49.2:8443: connect: connection refused" event="&Event{ObjectMeta:{kube-apiserver-minikube.1870185f2c73c7a8  kube-system   545 0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:kube-system,Name:kube-apiserver-minikube,UID:8312b4cdc4b705c0e12f63794469cfad,APIVersion:v1,ResourceVersion:,FieldPath:spec.containers{kube-apiserver},},Reason:Unhealthy,Message:Readiness probe failed: HTTP probe failed with statuscode: 500,Source:EventSource{Component:kubelet,Host:minikube,},FirstTimestamp:2025-10-20 04:24:11 +0000 UTC,LastTimestamp:2025-10-20 04:29:54.824320678 +0000 UTC m=+365.344063701,Count:3,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:minikube,}"
Oct 20 04:30:22 minikube kubelet[2209]: I1020 04:30:22.903060    2209 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8542e49c2001289366dfd174c2e0ef16e05a70b41ecad2526d99318a9aaeb1b1"
Oct 20 04:30:24 minikube kubelet[2209]: E1020 04:30:24.503450    2209 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://192.168.49.2:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="7s"
Oct 20 04:30:25 minikube kubelet[2209]: I1020 04:30:25.374050    2209 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="cf345086ed5efa7f4033b50c7c6c0f6cb97de270688fdd7ac46aa8395fda7953"
Oct 20 04:30:25 minikube kubelet[2209]: I1020 04:30:25.582405    2209 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3528f2fa14364860797133ebd7cafc7c409b9391637af73277162e08f3e9f869"
Oct 20 04:30:25 minikube kubelet[2209]: I1020 04:30:25.606701    2209 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="95778fc8ea1b993270ac055b9ca686aae9418ab0f77968adb5b37a1a0ec094ba"
Oct 20 04:30:26 minikube kubelet[2209]: I1020 04:30:26.482256    2209 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8593c8231d5224111266a338e21eda46c0b708fcc845e83da4ffc70375ac0af3"
Oct 20 04:30:29 minikube kubelet[2209]: E1020 04:30:29.701013    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="0c83f630-643b-414c-b6bb-294a9e466f53" pod="kube-system/storage-provisioner"
Oct 20 04:30:29 minikube kubelet[2209]: E1020 04:30:29.701417    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-proxy-2snft\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="0d495b3a-031f-44cf-8e2a-0d49e8c2dd03" pod="kube-system/kube-proxy-2snft"
Oct 20 04:30:29 minikube kubelet[2209]: E1020 04:30:29.701783    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/coredns-66bc5c9577-nhrn7\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="eea835d9-a82e-45f1-92ee-d99de871d32a" pod="kube-system/coredns-66bc5c9577-nhrn7"
Oct 20 04:30:29 minikube kubelet[2209]: E1020 04:30:29.702093    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="dc6cf0a7bcb54d1f95cecc4d7b6b7d67" pod="kube-system/kube-scheduler-minikube"
Oct 20 04:30:29 minikube kubelet[2209]: E1020 04:30:29.702314    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="3b51c8241e224d47681cce32ea99b407" pod="kube-system/kube-controller-manager-minikube"
Oct 20 04:30:29 minikube kubelet[2209]: E1020 04:30:29.702547    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="8312b4cdc4b705c0e12f63794469cfad" pod="kube-system/kube-apiserver-minikube"
Oct 20 04:30:29 minikube kubelet[2209]: E1020 04:30:29.702875    2209 status_manager.go:1018] "Failed to get status for pod" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused" podUID="e3a36fac0ae701bc11fad0a6716eec2c" pod="kube-system/etcd-minikube"
Oct 20 04:30:31 minikube kubelet[2209]: E1020 04:30:31.477752    2209 kubelet_node_status.go:486] "Error updating node status, will retry" err="failed to patch status \"{\\\"status\\\":{\\\"$setElementOrder/conditions\\\":[{\\\"type\\\":\\\"MemoryPressure\\\"},{\\\"type\\\":\\\"DiskPressure\\\"},{\\\"type\\\":\\\"PIDPressure\\\"},{\\\"type\\\":\\\"Ready\\\"}],\\\"conditions\\\":[{\\\"lastHeartbeatTime\\\":\\\"2025-10-20T04:30:31Z\\\",\\\"type\\\":\\\"MemoryPressure\\\"},{\\\"lastHeartbeatTime\\\":\\\"2025-10-20T04:30:31Z\\\",\\\"type\\\":\\\"DiskPressure\\\"},{\\\"lastHeartbeatTime\\\":\\\"2025-10-20T04:30:31Z\\\",\\\"type\\\":\\\"PIDPressure\\\"},{\\\"lastHeartbeatTime\\\":\\\"2025-10-20T04:30:31Z\\\",\\\"type\\\":\\\"Ready\\\"}],\\\"nodeInfo\\\":{\\\"containerRuntimeVersion\\\":\\\"docker://28.4.0\\\"}}}\" for node \"minikube\": Patch \"https://192.168.49.2:8443/api/v1/nodes/minikube/status?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 20 04:30:31 minikube kubelet[2209]: E1020 04:30:31.478001    2209 kubelet_node_status.go:486] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://192.168.49.2:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 20 04:30:31 minikube kubelet[2209]: E1020 04:30:31.478163    2209 kubelet_node_status.go:486] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://192.168.49.2:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 20 04:30:31 minikube kubelet[2209]: E1020 04:30:31.478317    2209 kubelet_node_status.go:486] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://192.168.49.2:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 20 04:30:31 minikube kubelet[2209]: E1020 04:30:31.478888    2209 kubelet_node_status.go:486] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://192.168.49.2:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
Oct 20 04:30:31 minikube kubelet[2209]: E1020 04:30:31.478920    2209 kubelet_node_status.go:473] "Unable to update node status" err="update node status exceeds retry count"
Oct 20 04:30:31 minikube kubelet[2209]: E1020 04:30:31.500373    2209 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://192.168.49.2:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="7s"
Oct 20 04:30:33 minikube kubelet[2209]: I1020 04:30:33.117545    2209 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0ab709de8ee2cf3c051de59108528bdb72f4aa9ad0bdf73963874fca1c8b3cf2"
Oct 20 04:30:37 minikube kubelet[2209]: E1020 04:30:37.770334    2209 status_manager.go:1018] "Failed to get status for pod" err="pods \"kube-scheduler-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" podUID="dc6cf0a7bcb54d1f95cecc4d7b6b7d67" pod="kube-system/kube-scheduler-minikube"
Oct 20 04:30:37 minikube kubelet[2209]: E1020 04:30:37.799693    2209 status_manager.go:1018] "Failed to get status for pod" err="pods \"kube-controller-manager-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" podUID="3b51c8241e224d47681cce32ea99b407" pod="kube-system/kube-controller-manager-minikube"
Oct 20 04:30:37 minikube kubelet[2209]: E1020 04:30:37.821221    2209 status_manager.go:1018] "Failed to get status for pod" err="pods \"kube-apiserver-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" podUID="8312b4cdc4b705c0e12f63794469cfad" pod="kube-system/kube-apiserver-minikube"
Oct 20 04:30:49 minikube kubelet[2209]: I1020 04:30:49.430142    2209 scope.go:117] "RemoveContainer" containerID="1e9f74e20398c13ce6a61c37ed686e108f57f0fbf4365b101db4a0b0595acfe5"
Oct 20 04:30:49 minikube kubelet[2209]: I1020 04:30:49.430594    2209 scope.go:117] "RemoveContainer" containerID="79e298e73d41c1361e2f85dba8ee64104fccd4fb8351c8be5258dcd102598fb9"
Oct 20 04:30:49 minikube kubelet[2209]: E1020 04:30:49.430758    2209 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(3b51c8241e224d47681cce32ea99b407)\"" pod="kube-system/kube-controller-manager-minikube" podUID="3b51c8241e224d47681cce32ea99b407"
Oct 20 04:30:56 minikube kubelet[2209]: I1020 04:30:56.746197    2209 scope.go:117] "RemoveContainer" containerID="79e298e73d41c1361e2f85dba8ee64104fccd4fb8351c8be5258dcd102598fb9"
Oct 20 04:30:56 minikube kubelet[2209]: E1020 04:30:56.746419    2209 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(3b51c8241e224d47681cce32ea99b407)\"" pod="kube-system/kube-controller-manager-minikube" podUID="3b51c8241e224d47681cce32ea99b407"
Oct 20 04:31:09 minikube kubelet[2209]: I1020 04:31:09.699918    2209 scope.go:117] "RemoveContainer" containerID="79e298e73d41c1361e2f85dba8ee64104fccd4fb8351c8be5258dcd102598fb9"


==> storage-provisioner [57fb1ab9e5dd] <==
W1020 04:31:51.712036       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:31:51.827225       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:31:53.945127       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:31:54.002607       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:31:56.007100       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:31:56.091061       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:31:58.095377       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:31:58.194910       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:00.195168       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:00.225006       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:02.242414       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:02.411301       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:04.415039       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:04.466828       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:06.471423       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:06.537389       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:08.542007       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:08.605714       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:10.609155       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:10.640564       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:12.645147       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:12.780400       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:14.783775       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:14.813917       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:16.817623       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:16.920726       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:18.923762       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:18.968949       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:20.972163       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:21.008919       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:23.012060       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:23.061087       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:25.065037       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:25.185515       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:27.189388       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:27.319646       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:29.321609       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:29.385609       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:31.388501       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:31.507970       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:33.516071       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:33.562408       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:35.566785       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:35.697175       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:37.700799       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:37.763955       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:39.767251       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:39.825363       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:41.829568       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:41.939858       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:44.383234       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:44.952558       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:46.956450       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:47.178793       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:49.449555       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:49.666235       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:51.670191       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:51.812885       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:53.937034       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:32:54.015979       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [ab59f9a8ea60] <==
W1020 04:28:49.259859       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:28:49.353005       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:28:51.356727       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:28:51.963344       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:28:53.967213       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:28:54.091998       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:28:56.095967       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:28:56.916543       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:28:58.918226       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:28:58.989097       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:01.086457       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:01.296531       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:03.300520       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:03.389290       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:05.393650       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:05.448013       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:07.455853       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:07.517939       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:09.521567       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:09.559800       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:11.563797       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:11.593479       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:13.597117       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:13.687758       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:15.690555       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:15.749549       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:17.821491       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:19.036968       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:21.041560       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:21.185636       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:23.189851       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:23.246026       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:25.250680       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:25.398497       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:27.485779       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:27.731082       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:29.738459       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:29.809477       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:31.818270       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:31.920640       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:33.933572       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:34.049167       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:36.053359       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:36.178696       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:38.183153       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:38.212465       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:40.216943       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:40.343062       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:42.347029       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:42.448789       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:44.453009       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:44.535238       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:46.538631       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:46.582797       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:48.587263       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:48.640069       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:50.643480       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:50.696326       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:52.980621       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1020 04:29:53.198295       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

